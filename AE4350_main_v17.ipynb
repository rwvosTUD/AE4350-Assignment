{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE4350 - Bio-inspired Intelligence and Learning Assignment\n",
    "This is the Main notebook for training and testing the DRL portfolio management system\n",
    "<br>Created on Thu May 12 13:22:50 2022\n",
    "<br>@author: Reinier Vos, 4663160-TUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_VERSION = 17\n",
    "try:\n",
    "    from utility_v17 import Agent, UtilFuncs, Statistics\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    from AE4350_Assignment.utility_v17 import Agent, UtilFuncs, Statistics\n",
    "except:\n",
    "    pass\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow as tf\n",
    "import plotly.graph_objects as pgo\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "SEED = 10\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"\\nATTENTION RUNNNING SCRIPT VERSION = {}\\n\".format(SCRIPT_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention:\n",
    "<br> Please run all cells in order unless advised otherwise\n",
    "<br> This version is the google colab compatible version, change next cell for desired environment\n",
    "\n",
    "##### Step 1) adjust the booleans in the following cell according to the environment you are working in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False # whether youre running the script in colab\n",
    "pay = False # whether youre using a payed colab version (i.e. own server)\n",
    "\n",
    "if using_colab:\n",
    "    from google.colab import files\n",
    "    if not pay:\n",
    "        from google.colab import drive\n",
    "        import shutil\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "    !git clone https://github.com/rwvosTUD/proj1.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System configuration definition\n",
    "##### Step 2) adjust the following configuration details according to your own experiment objective.\n",
    "<br> Notice that certain settings are experimental and should not be changed unless you have a specific reason to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"TEST\" # folder to save this runs results to\n",
    "\n",
    "modelsHyper_dct = {\"actor_ts_dLayers\":[256, 256, 256, 128, 64], # layers and hidden units for the actor's time series input \n",
    "                   \"actor_util_dLayers\":[], # \"\" for the actor's portfolio input \n",
    "                   \"actor_comb_dLayers\":[128, 128, 64, 64, 32], # \"\" for the actor's combined layers after concatenation  \n",
    "                   \"actor_regularizer\":1e-14, # Actor L2 regularization magnitude\n",
    "                   \"critic_ts_dLayers\":[256, 256, 256, 128, 64], # layers and hidden units for the actor's time series input \n",
    "                   \"critic_util_dLayers\":[], # \"\" for the actor's portfolio input\n",
    "                   \"critic_comb_dLayers\":[128, 128, 64, 64, 32], # \"\" for the critic's combined layers after concatenation  \n",
    "                   \"critic_action_dLayers\":[], # \"\" for the critic's action probability inputs\n",
    "                   \"critic_final_dLayers\":[128, 128, 64, 64, 32], # \"\" for the critic's final concatenated inputs\n",
    "                   \"critic_regularizer\":1e-14, # Critic L2 regularization magnitude\n",
    "                   \"use_batchNorm_tsdense\":True, # Whether to use batch normalization for every layer in the actor and critic TS layers \n",
    "                   \"use_dropout_tsdense\":True, # Whether to use dropout for every layer in the actor and critic TS layers \n",
    "                   \"ts_dropoutProb\":0.2, # Dropout probability, default 0.2\n",
    "}\n",
    "\n",
    "agent_dct = {\"stateTS_size\":64, # lookback window size for the time series state \n",
    "             \"stateUT_size\":6, # portfolio state size, do not adjust\n",
    "             \"batch_size\":128,\n",
    "             \"buffer_size\": 1000000, # replay buffer size\n",
    "             \"data_extraWindow\":0, # EXPERIMENTAL, do not change\n",
    "             \"n_budget\":1, # EXPERIMENTAL, do not change\n",
    "             \"is_terminal_threshold\":1000, # EXPERIMENTAL, do not change\n",
    "             \"model_hyper\": modelsHyper_dct, \n",
    "             \"train_tanh\":80, # training input scaling factor\n",
    "             \"vali_tanh\":104.26426426426426,# validation input scaling factor\n",
    "             \"test_tanh\":130.1301301301301,# test input scaling factor\n",
    "             \"gamma\":0.99, # discount factor for rewards\n",
    "             \"tau\":0.001, # soft update parameter\n",
    "             \"mask_input\":False, # EXPERIMENTAL, do not change\n",
    "             \"subset_training\":True, # whether to enable the trainer subsystem\n",
    "             \"subset_window\": 300, # trial size used when subset training\n",
    "}\n",
    "\n",
    "reward_dct = {\"rewardType\":7, # reward function to use, adviseable keep this at 7 which is the report's preferred function\n",
    "              \"penalty\":0, # EXPERIMENTAL, do not change\n",
    "              \"hold_scale\": 17, # exp sclaing factor for hold penalty\n",
    "              \"trade_scale\":14, # EXPERIMENTAL, do not change\n",
    "              \"trade_cost\":0, # transaction cost for a trade\n",
    "              \"max_holds\":100, # maximum holds after penalty is introduced, minimum hardcoded to 100!\n",
    "              \"prob_power\":1, # EXPERIMENTAL, do not change\n",
    "}\n",
    "trainer_dct = {\"EXTRACASH\" : reward_dct[\"trade_cost\"], \n",
    "            \"EXPAND\" : 10, # expansion size\n",
    "            \"LAST\" : 5, # amount of previous profits to consider for expansion timer\n",
    "            \"PROFITDIFF\" : 200, # minimum profit to be observed before expansion\n",
    "            \"EXPAND_TIMER\":  10, # how many runs have to exceed profitdiff threshold before expansion\n",
    "            \"TRADECOST_ACTUAL\" : 3, # actual trade cost\n",
    "             \"START_OFFSET\" : 300, # first subset window used by trainer\n",
    "             \"VALI_EC\":0, # extracash amount for validation\n",
    "             \"SCRIPT_VERSION\":SCRIPT_VERSION,\n",
    "             \"SEED\":SEED,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3) Initialize model and obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_extra = UtilFuncs.get_data(\"traindata\",agent_dct[\"data_extraWindow\"],\n",
    "                                      SCRIPT_VERSION, colab = using_colab,)\n",
    "l = len(data) - 1\n",
    "\n",
    "window_size = agent_dct[\"stateTS_size\"]\n",
    "data_extraWindow =  agent_dct[\"data_extraWindow\"]\n",
    "agent = Agent(agent_dct, data[window_size],\n",
    "              checkpoint_dir, reward_dct, trainer_dct) \n",
    "stats = Statistics(checkpoint_dir, training = True)\n",
    "stats_val = Statistics(checkpoint_dir, training = False)\n",
    "stats_rerun = Statistics(checkpoint_dir, training = False)\n",
    "print(\"=== ATTENTION: running model for {} stocks ===\".format(agent_dct[\"n_budget\"]))\n",
    "print(l)\n",
    "if agent.subset_training:\n",
    "    print(\"== Overview of first subset for training ==\")\n",
    "    plt.plot(data[window_size+agent.START_OFFSET:(agent.START_OFFSET+window_size+agent.subset_window)])\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of models\n",
    "##### Step 4) Load previous model's weights, replay buffer and other relevant histories\n",
    "Skip this step if no model is to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dir = \"runs/v17/content/v17_w80_p2\" \n",
    "    \n",
    "load_episode = 0\n",
    "agent.load_models(load_dir,load_episode, buffer = True, using_colab = using_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data loading & plotting\n",
    "##### Step 5) Load in validation data and evaluate it visually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_buyhold = UtilFuncs.plot_data(agent, data, data_extra, data_extraWindow, window_size, training = True)\n",
    "data_val, data_extra_val = UtilFuncs.get_data(\"validationdata\", data_extraWindow, SCRIPT_VERSION,  colab = using_colab)\n",
    "growth_buyhold_val = UtilFuncs.plot_data(agent, data_val, data_extra_val, data_extraWindow, window_size, training = False)\n",
    "data_val = np.append(data[-window_size:],data_val)\n",
    "l_val = len(data_val)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training & validation loop\n",
    "##### Step 6) Set last values before starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveIter = 30 # at which iteration multiple to save results\n",
    "start = 0 # at which epoch to start\n",
    "episode_count = 10000 # howmany epochs to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7) Train system and track results at bottom of cell\n",
    "Please be advised that training can be exhaustive and it can take > 600 epochs before the system is at \n",
    "a stage that out-of-sample performance will become decent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_figs = False\n",
    "debug = False\n",
    "testing = False\n",
    "#''' # uncomment this when resetting a trial \n",
    "reward_lst = []\n",
    "profitdiff_lst = []\n",
    "expansions_lst = []\n",
    "expand_i = 1\n",
    "\n",
    "history = {}\n",
    "history[\"training_profit\"] = []\n",
    "history[\"training_pratio\"]  = []\n",
    "history[\"validation_profit\"] = []\n",
    "history[\"validation_pratio\"] = []\n",
    "history[\"validation_extraCash\"] = []\n",
    "#'''\n",
    "deadlock_on = False\n",
    "timer = 0 \n",
    "expansion_size = 1 # start expansion_size at 1, will be set to EXPANSION after first\n",
    "use_terminateFunc = True\n",
    "terminateFunc_on = False\n",
    "#'''\n",
    "deadlock_probStart = 1/10 #1/6  # exploratory probability hack for actions 1 & 2\n",
    "decay = 0.90\n",
    "decay_op = 2-decay\n",
    "deadlock_prob = deadlock_probStart*(decay**(start)) # if we start later in an epoch\n",
    "switch_episode = 50\n",
    "#'''\n",
    "#extra_explore = False # whether to override the original exploration sheme\n",
    "#scaling = 0.1 #0.02\n",
    "tbzip_folder=\"/content/{0}_z.zip\".format(checkpoint_dir)\n",
    "zip_folder = \"/content/{0}\".format(checkpoint_dir)\n",
    "\n",
    "min_t = window_size \n",
    "stats.reset_all(agent.n_budget*data[window_size], growth_buyhold)\n",
    "stats_val.reset_all(agent.n_budget*data_val[window_size], growth_buyhold_val)\n",
    "print(\"\\n ATTENTION:\")\n",
    "#print(\"DEADLOCK IS SET TO {0}\".format(deadlock_on))\n",
    "print(\"SUBSET TRAINING = {0}\\n\".format(agent.subset_training))\n",
    "prev_terminate = True\n",
    "\n",
    "\n",
    "for e in range(start,episode_count):\n",
    "    agent.is_eval = False # training!\n",
    "    \n",
    "    if e % saveIter == 0 and e != 0:\n",
    "        episode_start = window_size\n",
    "        episode_end = l\n",
    "        agent.is_eval = True # over entire training set!\n",
    "    else:\n",
    "        ''' \n",
    "        SHUFFLING START \n",
    "        ''' \n",
    "        \n",
    "        \n",
    "        #deadlock_go = False\n",
    "        if (e-start) > agent.LAST and agent.subset_training:\n",
    "            profitdiff_mean = np.mean(profitdiff_lst[-agent.LAST:])\n",
    "\n",
    "            if profitdiff_mean <= 0:\n",
    "                # EXPONENTIALLY GROW\n",
    "                deadlock_prob = min(deadlock_prob*decay_op,deadlock_probStart) \n",
    "                timer = 0 # reset\n",
    "            elif profitdiff_mean > agent.PROFITDIFF:\n",
    "                # DECAY\n",
    "                deadlock_prob = max(deadlock_prob*decay,0.0001) \n",
    "                timer += 1\n",
    "            else: \n",
    "                timer = 0  # reset \n",
    "                \n",
    "            print(\"E{0} - Deadlock probability {1} | Timer = {2} | expansions = {3} | Prev mean = {4}\".format(e,\n",
    "                                                                         round(deadlock_prob,3), \n",
    "                                                                         timer,\n",
    "                                                                         expand_i-1,\n",
    "                                                                         profitdiff_mean))\n",
    "            if timer >= agent.EXPAND_TIMER: \n",
    "                expand_i += 1 \n",
    "                timer = 0\n",
    "                expansion_size = agent.EXPAND\n",
    "                #deadlock_go = True\n",
    "                \n",
    "        if agent.subset_training:\n",
    "            utils_start = [l,agent.START_OFFSET, agent.subset_window]\n",
    "            episode_start  = UtilFuncs.get_episodeStart(agent, expand_i, expansion_size, utils_start)\n",
    "            episode_end = episode_start+agent.subset_window\n",
    "            print(\"E{2} - Current window [{0},{1}]\".format(episode_start,episode_end,e))\n",
    "        else:\n",
    "            # dont use subset training!\n",
    "            episode_start = window_size\n",
    "            episode_end = l\n",
    "        \n",
    "    pbar = tqdm(total=(episode_end-episode_start), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "    \n",
    "    agent.reset(data[episode_start])\n",
    "    agent.balance += agent.EXTRACASH\n",
    "    stats.reset_episode()\n",
    "    stats.extraCash += agent.EXTRACASH \n",
    "\n",
    "    sold_price = 0\n",
    "    bought_price = agent.inventory_value\n",
    "    utils_state = [episode_end, stats.n_holds,stats.n_trades, agent.trade_cost, agent.train_tanh]\n",
    "    state = UtilFuncs.get_state(agent, data, episode_start, window_size + 1, utils_state)\n",
    "    \n",
    "    done = False\n",
    "    terminate = False\n",
    "    # =============================== ITERATION ======================================================\n",
    "    for t in range(episode_start,episode_end):\n",
    "\n",
    "        utils_act = []\n",
    "        action, action_prob = agent.take_action(state, utils_act)\n",
    "        \n",
    "        # DEADLOCK EXPERIMENT\n",
    "        action = UtilFuncs.break_deadlock(agent,action,e,utils_act, on = deadlock_on) \n",
    "            \n",
    "        # Hanle action\n",
    "        flags = [use_terminateFunc, terminateFunc_on]\n",
    "        utils_hdlAct = [action_prob]\n",
    "        action, profit,  impossible, terminate, term_msg = UtilFuncs.handle_action(agent, stats, action, data, \n",
    "                                                                                   t, flags, utils_hdlAct, training = True)\n",
    "        \n",
    "        # terminate if required\n",
    "        if terminate or t == episode_end-1:\n",
    "            # terminating i.e. agent.LAST iteration \n",
    "            done = True # terminal state has no 'next' so change done variable before use in next lines\n",
    "            \n",
    "        # get reward\n",
    "        if t < data_extraWindow:\n",
    "            ptn = data_extra[t]\n",
    "        else: \n",
    "            ptn = data[t-data_extraWindow]\n",
    "        utils_reward = [data[t],data[t-1], data[t+1], action, action_prob[0], stats.n_trades, stats.n_holds, impossible, l, terminate]\n",
    "        reward = agent.get_reward(agent, profit, utils_reward, done)\n",
    "        stats.total_reward += reward\n",
    "        \n",
    "        # take step\n",
    "        utils_state = [episode_end, stats.n_holds,stats.n_trades, agent.trade_cost, agent.train_tanh]\n",
    "        next_state = UtilFuncs.get_state(agent, data, t + 1, window_size + 1, utils_state)\n",
    "        if e % saveIter != 0 or e == 0: \n",
    "            actor_local_loss = agent.take_step(action_prob, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if terminate:\n",
    "            # ensure consistent length of arrays then terminate trial\n",
    "            if t >= min_t:\n",
    "                utils_pad = [l,t]\n",
    "                stats.pad_on_terminate(utils_pad)    \n",
    "            print(\"Episode {0} was terminated at {1}/{2} due to {3}\".format(e,t-window_size,episode_end-episode_start, term_msg))\n",
    "            break\n",
    "            \n",
    "        # collect \n",
    "        utils_saveIter = [profit, reward, actor_local_loss, action, t-episode_start]\n",
    "        stats.collect_iteration(agent,utils_saveIter)\n",
    "        \n",
    "        pbar.set_description(\"Episode {0}| Portfolio: {1}| Balance: {2}| Inventory: {3}| RewardAcc: {4}\".format(int(e),\n",
    "                                                                                                                UtilFuncs.to_currency(agent.balance+agent.inventory_value),\n",
    "                                                                                                                UtilFuncs.to_currency(agent.balance),\n",
    "                                                                                                                UtilFuncs.to_currency(agent.inventory_value),\n",
    "                                                                                                                UtilFuncs.to_currency(stats.total_reward)))\n",
    "        pbar.update()\n",
    "        \n",
    "    # ============================ END ITERATION =================================================\n",
    "    print(\"E{3} - Amount of +trades {0}, all trades {1} | ratio = {2}\".format(stats.n_posiProfits, \n",
    "                                                                              stats.n_trades, \n",
    "                                                                              round(stats.n_posiProfits/max(1,stats.n_trades),3), \n",
    "                                                                              e))\n",
    "\n",
    "    profitBuyhold = data[episode_end-1]-data[episode_start]\n",
    "    profitRL = agent.balance+agent.inventory_value-data[episode_start]-agent.TRADECOST_ACTUAL*stats.n_trades-stats.extraCash\n",
    "    profitdiff = profitRL-profitBuyhold\n",
    "    print(\"E{3} - RL profit = {0} | Buyhold = {1} | diff = {2} | tradecost = {4}\".format(round(profitRL,2),\n",
    "                                                                round(profitBuyhold,2),\n",
    "                                                                round(profitdiff,2),\n",
    "                                                                e,\n",
    "                                                                #agent.trade_cost*stats.n_trades,\n",
    "                                                                agent.TRADECOST_ACTUAL*stats.n_trades,\n",
    "                                                                ))\n",
    "    print(\"E{0} - impossibles {1}/{2} = {3}\".format(e, stats.n_impossible,\n",
    "                                             stats.n_1or2,\n",
    "                                             round(stats.n_impossible/stats.n_1or2,3)))\n",
    "    reward_lst.append(stats.total_reward)\n",
    "    profitdiff_lst.append(profitdiff) \n",
    "    expansions_lst.append(expand_i)\n",
    "\n",
    "    if max(e-1,0) % saveIter == 0 and e != 0 and e != 1 and not debug:\n",
    "        if not prev_terminate:\n",
    "            # we skip one iteration as we assume the zip file creation is done now\n",
    "            if using_colab and not pay: \n",
    "                shutil.copy(tbzip_folder, \"/content/drive/MyDrive\") # save result to drive\n",
    "    if e % saveIter != 0 or e == 0: \n",
    "        continue\n",
    "        \n",
    "    if True: #not terminate:\n",
    "        stats.collect_episode(agent,e, [])\n",
    "        history[\"training_profit\"].append(stats.compete[-1])\n",
    "        history[\"training_pratio\"].append(stats.n_posiProfits/max(1,stats.n_trades))\n",
    "        \n",
    "    if e % saveIter == 0 and e != 0: \n",
    "        if not debug:\n",
    "            # plot and save\n",
    "            agent.save_models(e)\n",
    "            utils_fig = [l, window_size]\n",
    "            stats.plot_figure(data, e, utils_fig, show_figs = show_figs)\n",
    "            stats.save_statistics(e)\n",
    "            extraHistory = {}\n",
    "            extraHistory[\"reward_lst\"] = reward_lst\n",
    "            extraHistory[\"profitdiff_lst\"] = profitdiff_lst\n",
    "            extraHistory[\"expansions_lst\"] = expansions_lst \n",
    "\n",
    "            with open(f'./{checkpoint_dir}/EXTRAhistory.json', 'w') as fp:\n",
    "                json.dump(extraHistory, fp)\n",
    "    \n",
    "            # ================ VALIDATION LOOP ===============================\n",
    "            stats_val.reset_episode()\n",
    "            stats_val.extraCash += agent.VALI_EC\n",
    "            agent.is_eval = True\n",
    "            agent.reset(data_val[window_size])\n",
    "            agent.balance += agent.VALI_EC\n",
    "            for t in trange(window_size,l_val):\n",
    "                utils_state = [l_val, stats_val.n_holds,stats_val.n_trades, agent.trade_cost, agent.vali_tanh]\n",
    "                state = UtilFuncs.get_state(agent, data_val, t, window_size + 1, utils_state)\n",
    "                utils_act = [deadlock_prob,data_val[t]]\n",
    "                action, action_prob = agent.take_action(state, utils_act)\n",
    "\n",
    "                flags = [use_terminateFunc, terminateFunc_on]\n",
    "                utils_hdlAct = [action_prob]\n",
    "                action, profit,  impossible, _, _ = UtilFuncs.handle_action(agent, stats_val, action, data_val, \n",
    "                                                                                           t, flags, utils_hdlAct, training = False)\n",
    "                utils_saveIter = [profit, 0., 0., action, t-window_size]\n",
    "                stats_val.collect_iteration(agent,utils_saveIter)\n",
    "\n",
    "            stats_val.collect_episode(agent, e, [])\n",
    "            print(\"Final validation profit = {0} | extra cash = {1} | +/all trades= {2}/{3}\".format(round(stats_val.compete[-1],2),\n",
    "                                                                            round(stats_val.extraCash,2),\n",
    "                                                                              stats_val.n_posiProfits,\n",
    "                                                                               stats_val.n_trades,))\n",
    "            utils_fig = [l_val, window_size]\n",
    "            stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)\n",
    "            history[\"validation_profit\"].append(stats_val.compete[-1])\n",
    "            history[\"validation_pratio\"].append(stats_val.n_posiProfits/max(1,stats_val.n_trades))\n",
    "            history[\"validation_extraCash\"].append(stats_val.extraCash)\n",
    "            # ============= END VALIDATION LOOP =============================\n",
    "            with open(f'./{checkpoint_dir}/e{e}/history.json', 'w') as fp:\n",
    "                json.dump(history, fp)\n",
    "            \n",
    "            \n",
    "        if using_colab and not debug:\n",
    "            !zip -r \"$tbzip_folder\" \"$zip_folder\"\n",
    "            print(\"Zip file created, saved next run\")\n",
    "        \n",
    "    if terminate:\n",
    "        prev_terminate = True\n",
    "    else:\n",
    "        prev_terminate = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf280",
   "language": "python",
   "name": "tf280"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
