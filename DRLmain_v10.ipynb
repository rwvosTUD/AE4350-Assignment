{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE4350 - Bio-inspired Intelligence and Learning Assignment\n",
    "This is the Main notebook for training and testing the DRL portfolio management system\n",
    "<br>Created on Thu May 12 13:22:50 2022\n",
    "<br>@author: Reinier Vos, 4663160-TUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from utility_v10 import Agent, UtilFuncs, Statistics\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    from AE4350_Assignment.utility_v10 import Agent, UtilFuncs, Statistics\n",
    "except:\n",
    "    pass\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "#In TensorFlow 2.3+, you can disable eager mode anytime using the following method:\n",
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(0)\n",
    "import plotly.graph_objects as pgo\n",
    "import time\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "#tf.config.run_functions_eagerly(False)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### README\n",
    "<br> Ensure utility v7 is used!\n",
    "<br> This version has is the google colab compatible version, change next cell for desired environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github link\n",
    "\n",
    "using_colab = False\n",
    "pay = False # whether youre using the payed version (i.e. own server)\n",
    "\n",
    "if using_colab:\n",
    "    from google.colab import files\n",
    "    if not pay:\n",
    "        from google.colab import drive\n",
    "        import shutil\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "    !git clone https://github.com/rwvosTUD/AE4350_Assignment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100 \n",
    "batch_size = 128 #32\n",
    "checkpoint_dir = \"runs/colab/content/v10_reg9_p1\" # \"run3_colab\"\n",
    "data_extraWindow = 1 # moving average window, do note that this will reduce the data length!\n",
    "data, data_extra = UtilFuncs.get_data(\"traindata\",data_extraWindow, colab = using_colab)\n",
    "l = len(data) - 1\n",
    "n_budget = 1 # budget based howmany assets you can buy at the start\n",
    "terminate_threshold = 1000\n",
    "hidden_units = [[128,128,64], [128,128, 64]] # [32, 64, 32, 64] default,first list for actor, second list for critic\n",
    "regularizer = 1e-9 # 1e-6 default\n",
    "reward_dct = {\"rewardType\":7,\n",
    "              \"penalty\":-1000,\n",
    "              \"hold_scale\":10,\n",
    "              \"trade_scale\":14,\n",
    "              \"trade_cost\":3,\n",
    "}\n",
    "\n",
    "\n",
    "agent = Agent(window_size, batch_size, hidden_units, regularizer,\n",
    "              data[window_size], n_budget, terminate_threshold,\n",
    "              checkpoint_dir, reward_dct, data_extraWindow = data_extraWindow) \n",
    "stats = Statistics(checkpoint_dir, training = True)\n",
    "stats_val = Statistics(checkpoint_dir, training = False)\n",
    "print(\"=== ATTENTION: running model for {} stocks ===\".format(n_budget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loading of models\n",
    "### General loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### loading of model\n",
    "#load_dir = \"runs/run3\"\n",
    "#agent.load_models(load_dir,40)\n",
    "#load_dir = \"runs/version3/run6_r5\"\n",
    "if using_colab:\n",
    "    load_dir = \"AE4350_Assignment/prev_runs/n{0}prev_run\".format(n_budget)\n",
    "else:\n",
    "    load_dir = \"runs/colab/content/v10_test_p1\" \n",
    "agent.load_models(load_dir,141)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrained actor loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor model loading\n",
    "actor_load_dir = \"actor\"\n",
    "agent.load_models(actor_load_dir,0, critic = False, actor = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_buyhold = UtilFuncs.plot_data(agent, data, data_extra, data_extraWindow, window_size, training = True)\n",
    "data_val, data_extra_val = UtilFuncs.get_data(\"validationdata\", data_extraWindow, colab = using_colab)\n",
    "l_val = len(data_val)-1\n",
    "growth_buyhold_val = UtilFuncs.plot_data(agent, data_val, data_extra_val, data_extraWindow, window_size, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_figs = False\n",
    "debug = False\n",
    "deadlock_on = False\n",
    "saveIter = 3\n",
    "start = 0\n",
    "episode_count = 3000 #300\n",
    "use_terminateFunc = True\n",
    "terminateFunc_on = False\n",
    "p_start = 1/6  # exploratory probability hack for actions 1 & 2\n",
    "decay = 0.99\n",
    "prob = p_start*(decay**(start)) # if we start later in an epoch\n",
    "switch_episode = 0\n",
    "#extra_explore = False # whether to override the original exploration sheme\n",
    "#scaling = 0.1 #0.02\n",
    "tbzip_folder=\"/content/{0}_z.zip\".format(checkpoint_dir)\n",
    "zip_folder = \"/content/{0}\".format(checkpoint_dir)\n",
    "\n",
    "min_t = window_size \n",
    "stats.reset_all(agent.n_budget*data[window_size], growth_buyhold)\n",
    "stats_val.reset_all(agent.n_budget*data_val[window_size], growth_buyhold_val)\n",
    "history = {}\n",
    "history[\"training\"] = []\n",
    "history[\"validation\"] = []\n",
    "\n",
    "prev_terminate = True\n",
    "for e in range(start,episode_count):\n",
    "    agent.is_eval = False # training!\n",
    "    if e >= switch_episode:\n",
    "        pbar = tqdm(total=(l-window_size), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "    #print(\"== Episode \" + str(e) + \"/\" + str(episode_count) +\" ==\")\n",
    "    agent.reset(data[window_size])\n",
    "    stats.reset_episode()\n",
    "    pt1 = data_extra[-1]\n",
    "    sold_price = 0\n",
    "    bought_price = agent.inventory_value\n",
    "    utils_state = [l, stats.n_holds,stats.n_trades, agent.trade_cost]\n",
    "    state = UtilFuncs.get_state(agent, data, window_size, window_size + 1, utils_state)\n",
    "    \n",
    "    done = False\n",
    "    terminate = False\n",
    "    \n",
    "    #REWARD TYPE SWITCH EXPERIMENT\n",
    "    #agent.switch_rewardType(7,switch_episode,e)\n",
    "\n",
    "    if not prev_terminate:\n",
    "        if deadlock_on:\n",
    "            prob = prob*decay # remove\n",
    "            print(\"E{0} - Deadlock probability {1}\".format(e,prob))\n",
    "    \n",
    "    # =============================== ITERATION ======================================================\n",
    "    for t in range(window_size,l):\n",
    "\n",
    "        utils_act = [prob,data[t]]\n",
    "        action, action_prob = agent.act(state, utils_act)\n",
    "        \n",
    "        # DEADLOCK EXPERIMENT\n",
    "        action = UtilFuncs.break_deadlock(agent,action,e,utils_act, on = deadlock_on) \n",
    "            \n",
    "        # Hanle action\n",
    "        flags = [use_terminateFunc, terminateFunc_on]\n",
    "        action, profit,  impossible, terminate, term_msg = UtilFuncs.handle_action(agent, stats, action, data, \n",
    "                                                                                   t, flags, training = True)\n",
    "        \n",
    "        # terminate if required\n",
    "        if terminate or t == l-1:\n",
    "            # terminating i.e. last iteration \n",
    "            done = True # terminal state has no 'next' so change done variable before use in next lines\n",
    "            \n",
    "        # get reward\n",
    "        if t < data_extraWindow:\n",
    "            ptn = data_extra[t]\n",
    "        else: \n",
    "            ptn = data[t-data_extraWindow]\n",
    "        utils_reward = [data[t],pt1, ptn, action, action_prob[0], stats.n_trades, stats.n_holds, impossible, l, terminate]\n",
    "        reward = agent.get_reward(agent, profit, utils_reward, done)\n",
    "        stats.total_reward += reward\n",
    "        pt1 = data[t]\n",
    "        \n",
    "        # take step\n",
    "        utils_state = [l, stats.n_holds,stats.n_trades, agent.trade_cost]\n",
    "        next_state = UtilFuncs.get_state(agent, data, t + 1, window_size + 1, utils_state)\n",
    "        actor_local_loss = agent.step(action_prob, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        # collect \n",
    "        utils_saveIter = [profit, reward, actor_local_loss, action, t-window_size]\n",
    "        stats.collect_iteration(agent,utils_saveIter)\n",
    "        \n",
    "        if terminate:\n",
    "            # ensure consistent length of arrays then terminate trial\n",
    "            if t >= min_t:\n",
    "                utils_pad = [l,t]\n",
    "                stats.pad_on_terminate(utils_pad)    \n",
    "            print(\"Episode {0} was terminated at {1}/{2} due to {3}\".format(e,t-window_size,l-window_size, term_msg))\n",
    "            break\n",
    "        if e >= switch_episode:\n",
    "            pbar.set_description(\"Episode {0}| Portfolio: {1}| Balance: {2}| Inventory: {3}| RewardAcc: {4}\".format(int(e),\n",
    "                                                                                                                    UtilFuncs.to_currency(agent.balance+agent.inventory_value),\n",
    "                                                                                                                    UtilFuncs.to_currency(agent.balance),\n",
    "                                                                                                                    UtilFuncs.to_currency(agent.inventory_value),\n",
    "                                                                                                                    UtilFuncs.to_currency(stats.total_reward)))\n",
    "            pbar.update()\n",
    "\n",
    "    # ============================ END ITERATION =================================================\n",
    "    print(\"E{3} - Amount of impossibles {0}, trades {1} | ratio = {2}\".format(stats.n_impossible, \n",
    "                                                                              stats.n_1or2-1, \n",
    "                                                                              round(stats.n_impossible/stats.n_1or2,3), \n",
    "                                                                              e))\n",
    "    if not terminate:\n",
    "        stats.collect_episode(agent,e, [])\n",
    "        history[\"training\"].append(stats.compete[-1])\n",
    "    \n",
    "        \n",
    "    if e % saveIter == 0 and e != 0 and not terminate:\n",
    "        if not debug:\n",
    "            # plot and save\n",
    "            agent.save_models(e)\n",
    "            utils_fig = [l, window_size]\n",
    "            stats.plot_figure(data, e, utils_fig, show_figs = show_figs)\n",
    "            stats.save_statistics(e)\n",
    "            \n",
    "            # ================ VALIDATION LOOP ===============================\n",
    "            stats_val.reset_episode()\n",
    "            agent.is_eval = True\n",
    "            agent.reset(data_val[window_size])\n",
    "            for t in trange(window_size,l_val):\n",
    "                utils_state = [l_val, stats_val.n_holds,stats_val.n_trades, agent.trade_cost]\n",
    "                state = UtilFuncs.get_state(agent, data_val, t, window_size + 1, utils_state)\n",
    "                utils_act = [prob,data_val[t]]\n",
    "                action, action_prob = agent.act(state, utils_act)\n",
    "\n",
    "                flags = [use_terminateFunc, terminateFunc_on]\n",
    "                action, profit,  impossible, terminate, term_msg = UtilFuncs.handle_action(agent, stats_val, action, data_val, \n",
    "                                                                                           t, flags, training = False)\n",
    "                utils_saveIter = [profit, 0., 0., action, t-window_size]\n",
    "                stats_val.collect_iteration(agent,utils_saveIter)\n",
    "\n",
    "            stats_val.collect_episode(agent, e, [])\n",
    "            print(\"Final validation profit = {0} & extra cash = {1}\".format(stats_val.compete[-1], stats_val.extraCash))\n",
    "            utils_fig = [l_val, window_size]\n",
    "            stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)\n",
    "            history[\"validation\"].append(stats_val.compete[-1])\n",
    "            # ============= END VALIDATION LOOP =============================\n",
    "            with open(f'./{checkpoint_dir}/e{e}/history.json', 'w') as fp:\n",
    "                json.dump(history, fp)\n",
    "            \n",
    "            \n",
    "        if using_colab and not debug:\n",
    "            !zip -r \"$tbzip_folder\" \"$zip_folder\"\n",
    "            print(\"Zip file created, saved next run\")\n",
    "        \n",
    "    if max(e-1,0) % saveIter == 0 and e != 0 and e != 1 and not debug:\n",
    "        if not prev_terminate:\n",
    "            # we skip one iteration as we assume the zip file creation is done now\n",
    "            if using_colab and not pay: \n",
    "                #!zip -r /content/run1_colab_r3v2.zip /content/run1_colab\n",
    "                #files.download(tbzip_folder)\n",
    "                shutil.copy(tbzip_folder, \"/content/drive/MyDrive\")\n",
    "                #b = 1\n",
    "                #print(\"Attempting download, system sleep initiated\")\n",
    "                #time.sleep(24) # Sleep system for x seconds, to give time for download\n",
    "                #print(\"System sleep terminated, continueing...\")\n",
    "\n",
    "    '''\n",
    "    EXPERIMENT\n",
    "    '''\n",
    "    if terminate:\n",
    "        prev_terminate = True\n",
    "    else:\n",
    "        prev_terminate = False\n",
    "            \n",
    "    #if debug and not terminate:\n",
    "    #    print(\"E{3} - Amount of impossibles {0} out of trades {1}, ratio = {2}\".format(n_impossible, n_1or2, n_impossible/n_1or2, e))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "history[\"validation\"] = []\n",
    "# ================ VALIDATION LOOP ===============================\n",
    "load_dir =  \"runs/colab/content/v10_big1\"\n",
    "files = os.listdir(load_dir)\n",
    "files = [list(map(int, re.findall('\\d+', file))) for file in files]\n",
    "files.sort()\n",
    "files = files[2:] # skip first two\n",
    "episodes = [x[0] for x in files]\n",
    "for e in episodes:\n",
    "    agent.load_models(load_dir,e)\n",
    "    stats_val.reset_episode()\n",
    "    agent.is_eval = True\n",
    "    agent.reset(data_val[window_size])\n",
    "    for t in trange(window_size,l_val):\n",
    "        utils_state = [l_val, stats_val.n_holds,stats_val.n_trades, agent.trade_cost]\n",
    "        state = UtilFuncs.get_state(agent, data_val, t, window_size + 1, utils_state)\n",
    "        utils_act = [prob,data_val[t]]\n",
    "        action, action_prob = agent.act(state, utils_act)\n",
    "\n",
    "        flags = [use_terminateFunc, terminateFunc_on]\n",
    "        action, profit,  impossible, terminate, term_msg = UtilFuncs.handle_action(agent, stats_val, action, data_val, \n",
    "                                                                                   t, flags, training = False)\n",
    "        utils_saveIter = [profit, 0., 0., action, t-window_size]\n",
    "        stats_val.collect_iteration(agent,utils_saveIter)\n",
    "\n",
    "    stats_val.collect_episode(agent, e, [])\n",
    "    print(\"Final validation profit = {0} & extra cash = {1}\".format(stats_val.compete[-1], stats_val.extraCash))\n",
    "    utils_fig = [l_val, window_size]\n",
    "    stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)\n",
    "    history[\"validation\"].append(stats_val.compete[-1])\n",
    "    # ============= END VALIDATION LOOP =============================\n",
    "        \n",
    "plt.plot(np.array(episodes),np.array(history[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(episodes),np.array(history[\"validation\"][82:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_val.reset_episode()\n",
    "agent.is_eval = True\n",
    "agent.reset(data_val[window_size])\n",
    "for t in trange(window_size,l_val):\n",
    "    utils_state = [l_val, stats_val.n_holds,stats_val.n_trades]\n",
    "    state = UtilFuncs.get_state(agent, data_val, t, window_size + 1, utils_state)\n",
    "    utils_act = [prob,data_val[t]]\n",
    "    action, action_prob = agent.act(state, utils_act)\n",
    "\n",
    "    flags = [use_terminateFunc, terminateFunc_on]\n",
    "    action, profit,  impossible, terminate, term_msg = UtilFuncs.handle_action(agent, stats_val, action, data_val, \n",
    "                                                                               t, flags, training = False)\n",
    "    utils_saveIter = [profit, 0., 0., action]\n",
    "    stats_val.collect_iteration(agent,utils_saveIter)\n",
    "\n",
    "stats_val.collect_episode(agent, e, [])\n",
    "utils_fig = [l_val, window_size]\n",
    "stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = agent.memory\n",
    "with open(f'./memoryBIG.npy', 'wb') as f:\n",
    "    np.save(f, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats_val.collect_episode(agent, e, [])\n",
    "utils_fig = [l_val, window_size]\n",
    "stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final save\n",
    "agent.save_models(0)\n",
    "if using_colab:\n",
    "    #!zip -r /content/run1_colab_r3v2.zip /content/run1_colab\n",
    "    !zip -r \"$tbzip_folder\" \"$zip_folder\"\n",
    "    files.download(tbzip_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually save results\n",
    "'''\n",
    "with open(f'./models/{runName}/{runName}_parameters.txt', 'w') as f:\n",
    "    print(allParameters, file=f)\n",
    "\n",
    "# Saving results\n",
    "with open(f'./{checkpoint_dir}/results/total_rewards.npy', 'wb') as f:\n",
    "    np.save(f, np.array(total_reward_list))\n",
    "with open(f'./{checkpoint_dir}/results/EveryProfit_dct.json', 'w') as fp:\n",
    "    json.dump(EveryProfit_dct, fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading results\n",
    "'''\n",
    "with open(f'./{checkpoint_dir}/total_rewards.npy', 'rb') as f:\n",
    "    total_reward_list = np.load(f)\n",
    "with open(f'./{checkpoint_dir}/EveryProfit_dct.json', 'r') as fp:\n",
    "    EveryProfit_dct = json.load(fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup naive benchmark, buy and hold for training data\n",
    "test_data, test_data_extra = UtilFuncs.get_data(\"testdata\", data_extraWindow)\n",
    "data = test_data\n",
    "data_extra = test_data_extra\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(test_data)), y=test_data,\n",
    "                    mode='lines',\n",
    "                    name='stock growth'))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(test_data_extra)), y=test_data_extra,\n",
    "                    mode='lines',\n",
    "                    name='stock MA adjusted W ={}'.format(data_extraWindow)))\n",
    "fig.show()\n",
    "growth_buyhold_per = (data[-1]-data[window_size])/data[window_size]\n",
    "\n",
    "print(\"Naive buy & hold strategy on TEST data has a portfolio growth of {0}% per asset bought at the start of 2015 and sold at 2018\".format(round(growth_buyhold_per,3)))\n",
    "growth_buyhold_cash = agent.n_budget*data[window_size]*growth_buyhold_per\n",
    "growth_buyhold = (agent.n_budget*(data-data[window_size]))[window_size:-1]\n",
    "print(\"For current budget of {0}, this means that for {1} stocks bought in 2015 and sold in 2018 one achieves a final portfolio growth of {2}\".format(UtilFuncs.to_currency(agent.n_budget*data[window_size]),agent.n_budget,UtilFuncs.to_currency(growth_buyhold_cash)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dir = \"runs/colab/content/run1_colab_noPTcont\"\n",
    "load_dir = \"runs/colab/content/run1_v6_prftAdj+holdbonus25\"\n",
    "try:\n",
    "    agent.setup_validation(load_dir)\n",
    "except:\n",
    "    pass\n",
    "checkpoint_dir = load_dir\n",
    "val_episode = 20\n",
    "agent.load_models(load_dir,val_episode)\n",
    "agent.load_attributes(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation cell\n",
    "data = test_data # sanity check\n",
    "data_extra = test_data_extra\n",
    "l = len(data) - 1\n",
    "l1 = 2263\n",
    "agent.is_eval = True #True\n",
    "extraCash = 0\n",
    "showfigs = True\n",
    "deadlock_on = False # must remain off!\n",
    "debug = False\n",
    "use_terminateFunc = True\n",
    "terminateFunc_on = False\n",
    "prob = 0 # not used\n",
    "# initialize and container setup\n",
    "agent.reset(data[window_size])\n",
    "agent.validation_extraCash(extraCash) # function must be after reset!\n",
    "total_reward = 0 # total profit resets every epsiode \n",
    "n_trades = 0 \n",
    "n_impossible = 0 \n",
    "n_holds = 0 \n",
    "n_1or2 = 1 # 1 not zero because we cant have division by zero \n",
    "profits = []\n",
    "balances = []\n",
    "rewards = []\n",
    "inventories = [] # inventory value (only stocks)\n",
    "actor_local_losses = []\n",
    "buy_ind = []\n",
    "sell_ind = []\n",
    "utils_state = [l1, n_holds, n_trades]\n",
    "state = UtilFuncs.get_state(agent, data, window_size, window_size + 1, utils_state)\n",
    "pt1 = data_extra[-1]\n",
    "\n",
    "pbar = tqdm(total=l, position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "done = False\n",
    "terminate = False\n",
    "\n",
    "for t in range(window_size,l):\n",
    "    profit = 0 # initialize\n",
    "    change = 0 \n",
    "    impossible = False\n",
    "    utils_act = [prob,data[t]]\n",
    "    action, action_prob = agent.act(state, utils_act)\n",
    "\n",
    "    # DEADLOCK EXPERIMENT\n",
    "    #action = UtilFuncs.break_deadlock(agent,action,e,utils_act, on = deadlock_on) \n",
    "\n",
    "    # =========================== ACTION SPACE ======================================\n",
    "    if action == 0:\n",
    "        n_holds += 1\n",
    "\n",
    "    elif action == 1:\n",
    "        n_1or2 += 1\n",
    "        \n",
    "        if len(agent.inventory) == 0: #max one stock \n",
    "            if agent.balance > data[t]:\n",
    "                # BUYING stock, only if there is balance though\n",
    "                agent.inventory.append(data[t])\n",
    "                sold_price = agent.inventory_conj.pop(0)\n",
    "\n",
    "                profit = sold_price - data[t]\n",
    "\n",
    "                change = -data[t]\n",
    "                buy_ind.append(t)\n",
    "                n_trades += 1\n",
    "                n_holds = 0 # reset counter\n",
    "            else: \n",
    "                extraCash += data[t] - agent.balance\n",
    "                agent.reset(data[t])\n",
    "        else:\n",
    "            impossible = True\n",
    "            n_impossible += 1\n",
    "            n_holds += 1 # effectively no buy is a hold\n",
    "            if not use_terminateFunc:\n",
    "                terminate = True\n",
    "\n",
    "    elif action == 2:\n",
    "        n_1or2 += 1\n",
    "        if len(agent.inventory) > 0: \n",
    "            # SELLING stock, only if there are stocks held\n",
    "\n",
    "            bought_price = agent.inventory.pop(0)\n",
    "            agent.inventory_conj.append(data[t])\n",
    "\n",
    "            profit = data[t] - bought_price \n",
    "\n",
    "            change = data[t]\n",
    "            sell_ind.append(t)\n",
    "            n_trades += 1\n",
    "            n_holds = 0 # reset counter\n",
    "        else:\n",
    "            impossible = True\n",
    "            n_impossible += 1\n",
    "            n_holds += 1 # effectively no sell is a hold\n",
    "            if not use_terminateFunc:\n",
    "                terminate = True\n",
    "    # =============================== END ======================================\n",
    "\n",
    "    agent.update_balance(change)\n",
    "    agent.update_inventory(data[t])\n",
    "    if use_terminateFunc:\n",
    "        utils_term = [n_impossible, np.min(data[(t+1):])]\n",
    "        terminate = agent.check_threshold(utils_term, terminateFunc_on= terminateFunc_on)\n",
    "        #terminate = agent.check_threshold()\n",
    "    if terminate or t == l-1:\n",
    "        # terminating i.e. last iteration \n",
    "        done = True # terminal state has no 'next' so change done variable before use in next lines\n",
    "\n",
    "\n",
    "    if t < data_extraWindow:\n",
    "        ptn = data_extra[t]\n",
    "    else: \n",
    "        ptn = data[t-data_extraWindow]\n",
    "    utils_reward = [data[t],pt1, ptn, action, n_trades, n_holds, impossible, l, terminate]\n",
    "    pt1 = data[t]\n",
    "    '''\n",
    "    change names of variables if it works\n",
    "    '''\n",
    "    reward = agent.get_reward(agent, profit, utils_reward, done)\n",
    "    total_reward += reward\n",
    "\n",
    "    utils_state = [l, n_holds,n_trades]\n",
    "    next_state = UtilFuncs.get_state(agent, data, t + 1, window_size + 1, utils_state)\n",
    "\n",
    "    #actor_local_loss = agent.step(action_prob, reward, next_state, done)\n",
    "    state = next_state\n",
    "\n",
    "    # collect \n",
    "    balances.append(agent.balance)\n",
    "    inventories.append(agent.inventory_value)\n",
    "    profits.append(profit)\n",
    "    rewards.append(reward)\n",
    "    #actor_local_losses.append(float(actor_local_loss))\n",
    "    \n",
    "    if terminate:\n",
    "        # ensure consistent length of arrays then terminate trial\n",
    "        if t >= 1500:\n",
    "            balances = np.pad(balances,(0,l-t-1),'constant',constant_values=(0,balances[-1])).tolist()\n",
    "            inventories = np.pad(inventories,(0,l-t-1),'constant',constant_values=(0,inventories[-1])).tolist()\n",
    "            profits = np.pad(profits,(0,l-t-1),'constant',constant_values=(0,0)).tolist()\n",
    "            rewards = np.pad(rewards,(0,l-t-1),'constant',constant_values=(0,0)).tolist()\n",
    "            #actor_local_losses = np.pad(actor_local_losses,(0,l-t-1),'constant',constant_values=(0,actor_local_losses[-1])).tolist()\n",
    "        print(\"Episode {0} was terminated at {1}/{2} due to terminal state\".format('test',t,l))\n",
    "        break\n",
    "\n",
    "    pbar.set_description(\"VALIDATION e{4}| Portfolio: {0}| Balance: {1}| Inventory: {2}| RewardAcc: {3}\".format(UtilFuncs.to_currency(agent.balance+agent.inventory_value),\n",
    "                                                                                                            UtilFuncs.to_currency(agent.balance),\n",
    "                                                                                                            UtilFuncs.to_currency(agent.inventory_value),\n",
    "                                                                                                            UtilFuncs.to_currency(total_reward),\n",
    "                                                                                                            val_episode))\n",
    "    pbar.update()\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    # collect & save\n",
    "growth = (np.array(balances)+np.array(inventories)-agent.budget).tolist() # \n",
    "compete = (np.array(growth)-growth_buyhold).tolist() # compete vs buyhold\n",
    "print(\"Amount of impossibles {0}, trades {1} (+1!)| ratio = {2}\".format(n_impossible, n_1or2, n_impossible/n_1or2))\n",
    "fig.update_layout(showlegend=True, xaxis_range=[window_size, l], \n",
    "                  title_text = \"Test e{5} | profit RL: {0} vs buyhold: {1}, difference = {2} | extra cash = {3}| Impossible ratio {4}\".format(round(growth[-1],2),\n",
    "                                                                                                                     round(growth_buyhold[-1],2),\n",
    "                                                                                                                     round(growth[-1]-growth_buyhold[-1],2),\n",
    "                                                                                                                     extraCash,\n",
    "                                                                                                                     round(n_impossible/n_1or2,3),\n",
    "                                                                                                                     val_episode))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(data)), y=data,\n",
    "                    mode='lines',\n",
    "                    name='data'))\n",
    "fig.add_trace(pgo.Scatter(x=buy_ind, y=data[buy_ind], marker_color = \"green\",\n",
    "                    mode='markers',\n",
    "                    name='buy'))\n",
    "fig.add_trace(pgo.Scatter(x=sell_ind, y=data[sell_ind], marker_color = \"red\",\n",
    "                    mode='markers',\n",
    "                    name='sell'))\n",
    "if showfigs:\n",
    "    fig.show(render = \"browser\")\n",
    "fig.write_html(\"./{0}/validation/e{1}_tradesTest.html\".format(checkpoint_dir,val_episode))\n",
    "    \n",
    "# collect & save\n",
    "save_df = {\"balance\":balances,\"inventory\":inventories,\"profit\":profits,\"reward\":rewards,\"growth\":growth,\"compete\":compete,\"growth_buyhold\":growth_buyhold.tolist()}\n",
    "save_df = pd.DataFrame(save_df).to_csv(\"./{0}/validation/e{1}_testResults.csv\".format(checkpoint_dir,val_episode))\n",
    "print(\"Succesfully saved csv file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraCash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_state = [l, n_holds]\n",
    "t = 99\n",
    "state = UtilFuncs.get_state(agent, data, t, window_size + 1, utils_state)\n",
    "utils_act = [prob,data[t]]\n",
    "state[0][0:] = 0.5 \n",
    "state[0][-5] = -0.2 # bal\n",
    "state[0][-4] = -0.3 # invent\n",
    "state[0][-3] = -0.3 # port\n",
    "state[0][-2] = 0.8 # hold\n",
    "state[0][-1] = 0. # holdin\n",
    "print(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, action_prob = agent.act(state, utils_act)\n",
    "action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting RL vs buyhold\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(growth)), y=np.array(growth),\n",
    "                    mode='lines',\n",
    "                    name='growth_RL'))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(growth)), y=np.array(growth_buyhold),\n",
    "                    mode='lines',\n",
    "                    name='growth_buyhold'))\n",
    "fig.add_trace(pgo.Scatter(x=buy_ind, y=data[buy_ind], marker_color = \"green\",\n",
    "                    mode='markers',\n",
    "                    name='buy'))\n",
    "fig.add_trace(pgo.Scatter(x=sell_ind, y=data[sell_ind], marker_color = \"red\",\n",
    "                    mode='markers',\n",
    "                    name='sell'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing whether model even learnt\n",
    "model1 = Agent(window_size, batch_size)\n",
    "model1 = model1.critic_target.model\n",
    "a= model1.get_weights()\n",
    "print(a[10])\n",
    "print(\"=================================\")\n",
    "model2 = Agent(window_size, batch_size)\n",
    "model2.critic_target.model.load_weights(\"./run1/e203/critic_local.h5\")\n",
    "b =model2.critic_target.model.get_weights()\n",
    "print(b[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "growth_buyhold[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
