{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE4350 - Bio-inspired Intelligence and Learning Assignment\n",
    "This is the Main notebook for training and testing the DRL portfolio management system\n",
    "<br>Created on Thu May 12 13:22:50 2022\n",
    "<br>@author: Reinier Vos, 4663160-TUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utility_v5 import Agent, getData, getState, formatPrice\n",
    "from AE4350_Assignment.utility_v5 import Agent, getData, getState, formatPrice\n",
    "import sys\n",
    "from tqdm import tqdm # gives progress bar when loading\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "#In TensorFlow 2.3+, you can disable eager mode anytime using the following method:\n",
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(0)\n",
    "import plotly.graph_objects as pgo\n",
    "import time\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "#tf.config.run_functions_eagerly(False)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### README\n",
    "<br> Ensure utility v5 is used!\n",
    "<br> This version has is the google colab compatible version, change next cell for desired environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github link\n",
    "\n",
    "using_colab = False\n",
    "\n",
    "\n",
    "if using_colab:\n",
    "    from google.colab import files\n",
    "    !git clone https://github.com/rwvosTUD/AE4350_Assignment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models will be saved to C:\\Users\\Reinier Vos\\Documents\\YEAR2021_2022\\AE MSc C&S\\AE4350-Bio_inspiredIntelligenceAndLearning\\assignment\\scripts\\run3_colab\n",
      "Reward function description: terminal reward and penalty for buy hold, no intermediary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reinier Vos\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully saved model parameters to folder run3_colab\n"
     ]
    }
   ],
   "source": [
    "window_size = 100 \n",
    "batch_size = 64 #32\n",
    "checkpoint_dir = \"run3_colab\"\n",
    "data_extraWindow = 1 # moving average window, do note that this will reduce the data length!\n",
    "data, data_extra = getData(\"traindata\",data_extraWindow, colab = using_colab)\n",
    "l = len(data) - 1\n",
    "episode_count = 300\n",
    "n_budget = 5 # budget based howmany assets you can buy at the start\n",
    "terminate_threshold = 0.10\n",
    "hidden_units = [32, 64, 32, 64] # [32, 64, 32, 64] default,first 2 for actor, last 2 for critic\n",
    "regularizer = 1e-6 # 1e-6 default\n",
    "\n",
    "\n",
    "agent = Agent(window_size, batch_size, hidden_units, regularizer,\n",
    "              data[0], n_budget, terminate_threshold,\n",
    "              checkpoint_dir, rewardType = 3, data_extraWindow = data_extraWindow) \n",
    "print(\"=== ATTENTION: running model for {} stocks ===\".format(n_budget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'C:\\Users\\Reinier Vos\\Documents\\YEAR2021_2022\\AE MSc C&S\\AE4350-Bio_inspiredIntelligenceAndLearning\\assignment\\scripts\\runs/run1\\e90\\actor_local.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c78689804480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mload_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"runs/run1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\YEAR2021_2022\\AE MSc C&S\\AE4350-Bio_inspiredIntelligenceAndLearning\\assignment\\scripts\\utility_v5.py\u001b[0m in \u001b[0;36mload_models\u001b[1;34m(self, checkpoint_dir, episode)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'e{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'actor_local.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'e{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'actor_target.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'e{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'critic_local.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'C:\\Users\\Reinier Vos\\Documents\\YEAR2021_2022\\AE MSc C&S\\AE4350-Bio_inspiredIntelligenceAndLearning\\assignment\\scripts\\runs/run1\\e90\\actor_local.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "### loading of model\n",
    "#load_dir = \"runs/run3\"\n",
    "#agent.load_models(load_dir,40)\n",
    "#load_dir = \"runs/version3/run6_r5\"\n",
    "if using_colab:\n",
    "    load_dir = \"AE4350_Assignment/prev_runs/n{0}prev_run\".format(n_budget)\n",
    "else:\n",
    "    load_dir = \"runs/run1\"\n",
    "agent.load_models(load_dir,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup naive benchmark, buy and hold for training data\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True, title_text =\"Training data\")\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(data)), y=data,\n",
    "                    mode='lines',\n",
    "                    name='stock growth'))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(data_extra)), y=data_extra,\n",
    "                    mode='lines',\n",
    "                    name='stock MA adjusted W ={}'.format(data_extraWindow)))\n",
    "fig.show()\n",
    "growth_buyhold_per = (data[-1]-data[0])/data[0]\n",
    "\n",
    "print(\"Naive buy & hold strategy on TRAINIING data has a portfolio growth of {0}% per asset bought at the start of 2006 and sold at 2015\".format(round(growth_buyhold_per,3)))\n",
    "growth_buyhold_cash = agent.budget*growth_buyhold_per\n",
    "growth_buyhold = (agent.n_budget*(data-data[0]))[0:-1]\n",
    "print(\"For current budget of {0}, this means {1} stocks bought in 2006 result in a final portfolio growth of {2} (i.e. final value ={3})\".format(formatPrice(agent.budget),\n",
    "                                                                                                                                                 agent.n_budget,\n",
    "                                                                                                                                                 formatPrice(growth_buyhold_cash),\n",
    "                                                                                                                                                 formatPrice(growth_buyhold_cash+agent.budget)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "showfigs = False\n",
    "saveIter = 10\n",
    "start = 0\n",
    "scaling = 0.1 #0.02\n",
    "tbzip_folder=\"/content/{0}_z.zip\".format(checkpoint_dir)\n",
    "zip_folder = \"/content/{0}\".format(checkpoint_dir)\n",
    "\n",
    "#pbar = tqdm(total=l*episode_count, position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "#pbar = tqdm(total=episode_count, position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "totalReward_list = []\n",
    "lastLosses_list=[]\n",
    "everyProfit_dct = {}\n",
    "everyBalance_dct = {}\n",
    "everyReward_dct = {}\n",
    "everyInventory_dct = {}\n",
    "everyGrowth_dct = {}\n",
    "everyGrowth_dct[\"buyhold\"] = growth_buyhold.tolist()\n",
    "everyCompete_dct = {}\n",
    "everyLoss_dct = {}\n",
    "agent.is_eval = False\n",
    "\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "for e in range(start,episode_count):\n",
    "    pbar = tqdm(total=l, position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "    #print(\"== Episode \" + str(e) + \"/\" + str(episode_count) +\" ==\")\n",
    "    agent.reset(data[0])\n",
    "    state = getState(agent, data, 0, window_size + 1)\n",
    "\n",
    "    total_reward = 0 # total profit resets every epsiode \n",
    "     \n",
    "    profits = []\n",
    "    balances = []\n",
    "    rewards = []\n",
    "    inventories = [] # inventory value (only stocks)\n",
    "    actor_local_losses = []\n",
    "    buy_ind = []\n",
    "    sell_ind = []\n",
    "    pt1 = data_extra[-1]\n",
    "    \n",
    "    done = False\n",
    "    for t in range(l):\n",
    "        profit = 0 # initialize\n",
    "        change = 0 \n",
    "        sale_profit = 0\n",
    "        action, action_prob = agent.act(state)\n",
    "        next_state = getState(agent, data, t + 1, window_size + 1)\n",
    "\n",
    "        if action == 1 and agent.balance > data[t]:\n",
    "            # buying stock, only if there is balance though\n",
    "            agent.inventory.append(data[t])\n",
    "            #agent.inventory_ma.append(data_extra[t])\n",
    "            change = -data[t]\n",
    "            buy_ind.append(t)\n",
    "            \n",
    "        elif action == 2 and len(agent.inventory) > 0:\n",
    "            # selling stock, only if there are stocks held\n",
    "\n",
    "            bought_price = agent.inventory.pop(0)\n",
    "            #sale_profit = data[t]-bought_price\n",
    "            profit = data[t] - bought_price \n",
    "            \n",
    "            #profit = agent.balance + agent.inventory_value - growth_buyhold[t] # profit vs buyhold\n",
    "            '''\n",
    "            _ = agent.inventory.pop(0)\n",
    "            bought_price = agent.inventory_ma.pop(0)\n",
    "            profit = data_extra[t]-bought_price\n",
    "            '''\n",
    "            change = data[t]\n",
    "            sell_ind.append(t)\n",
    "       \n",
    "        '''\n",
    "        else:\n",
    "            # hold stocks\n",
    "            profit = 0 # explicit reset\n",
    "            change = 0\n",
    "            terminate = agent.update_balance(0,inventory_value) # check portfolio based on stock changes\n",
    "        # all statement done in else loop are initialized at start of iteration\n",
    "        '''\n",
    "        agent.update_balance(change)\n",
    "        agent.update_inventory(data[t])\n",
    "        terminate = agent.check_threshold()\n",
    "        if terminate or t == l-1:\n",
    "            # terminating i.e. last iteration \n",
    "            done = True # terminal state has no 'next' so change done variable before use in next lines\n",
    "            \n",
    "        '''\n",
    "        ================= PROFIT EXPERIMENT ===================\n",
    "        we reward points whenever the portfolio is above buyhold! sort of cartpole balancing point awarding \n",
    "        we clip it at max(0,a) because we dont want to overfit!\n",
    "\n",
    "        diff = agent.balance + agent.inventory_value - data[t]*agent.n_budget\n",
    "        if diff > 0:\n",
    "            #portfolio worth more than buy hold, hold this position\n",
    "            profit = sale_profit #diff*scaling+sale_profit # cartpole balancing term + risk term\n",
    "        else:\n",
    "        '''\n",
    "        #portfolio worth less than buy hold, try to make up; \n",
    "        #notice we dont look at the portfolio value anymore\n",
    "        '''\n",
    "            profit = sale_profit # only risk term\n",
    "            # add penalty term? or maybe inflate balance in order to make it more risky\n",
    "           \n",
    "        \n",
    "        ======================================================\n",
    "        '''\n",
    "        #price = data[t]\n",
    "        \n",
    "        \n",
    "        if t < data_extraWindow:\n",
    "            ptn = data_extra[t]\n",
    "        else: \n",
    "            ptn = data[t-data_extraWindow]\n",
    "        util_lst = [data[t],pt1, ptn, action]\n",
    "        pt1 = data[t]\n",
    "        '''\n",
    "        change names of variables if it works\n",
    "        '''\n",
    "        reward = agent.get_reward(agent, profit, util_lst, done)\n",
    "        total_reward += reward\n",
    "        actor_local_loss = agent.step(action_prob, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        # collect \n",
    "        balances.append(agent.balance)\n",
    "        inventories.append(agent.inventory_value)\n",
    "        profits.append(profit)\n",
    "        rewards.append(reward)\n",
    "        actor_local_losses.append(float(actor_local_loss))\n",
    "        \n",
    "        if terminate:\n",
    "            # ensure consistent length of arrays then terminate trial\n",
    "            balances = np.pad(balances,(0,l-t-1),'constant',constant_values=(0,balances[-1])).tolist()\n",
    "            inventories = np.pad(inventories,(0,l-t-1),'constant',constant_values=(0,inventories[-1])).tolist()\n",
    "            profits = np.pad(profits,(0,l-t-1),'constant',constant_values=(0,0)).tolist()\n",
    "            rewards = np.pad(rewards,(0,l-t-1),'constant',constant_values=(0,0)).tolist()\n",
    "            actor_local_losses = np.pad(rewards,(0,l-t-1),'constant',constant_values=(0,actor_local_loss[-1])).tolist()\n",
    "            print(\"Episode {0} was terminated at {1}/{2} due to terminal balance state\".format(e,t,l))\n",
    "            break\n",
    "        \n",
    "        pbar.set_description(\"Episode {0}| Portfolio: {1}| Balance: {2}| Inventory: {3}| RewardAcc: {4}\".format(int(e),\n",
    "                                                                                                                formatPrice(agent.balance+agent.inventory_value),\n",
    "                                                                                                                formatPrice(agent.balance),\n",
    "                                                                                                                formatPrice(agent.inventory_value),\n",
    "                                                                                                                formatPrice(total_reward)))\n",
    "        pbar.update()\n",
    "\n",
    "        \n",
    "    # collect & save\n",
    "    growth = (np.array(balances)+np.array(inventories)-agent.budget).tolist() # \n",
    "    compete = (np.array(growth)-growth_buyhold).tolist() # compete vs buyhold\n",
    "    \n",
    "    totalReward_list.append(total_reward)\n",
    "    lastLosses_list.append(actor_local_loss)\n",
    "    everyProfit_dct[\"e{}\".format(e)] = profits\n",
    "    everyBalance_dct[\"e{}\".format(e)] = balances\n",
    "    everyReward_dct[\"e{}\".format(e)] = rewards\n",
    "    everyInventory_dct[\"e{}\".format(e)] = inventories\n",
    "    everyGrowth_dct[\"e{}\".format(e)] = growth\n",
    "    everyCompete_dct[\"e{}\".format(e)] = compete\n",
    "    everyLoss_dct[\"e{}\".format(e)] = actor_local_losses\n",
    "    \n",
    "    if e % saveIter == 0 and e != 0:\n",
    "        agent.save_models(e)\n",
    "        with open(f'./{checkpoint_dir}/e{e}/totalReward.npy', 'wb') as f:\n",
    "            np.save(f, np.array(totalReward_list))\n",
    "        with open(f'./{checkpoint_dir}/e{e}/Losses.npy', 'wb') as f:\n",
    "            np.save(f, np.array(lastLosses_list))\n",
    "        with open(f'./{checkpoint_dir}/e{e}/EveryProfit.json', 'w') as fp:\n",
    "            json.dump(everyProfit_dct, fp)\n",
    "        with open(f'./{checkpoint_dir}/e{e}/EveryBalance.json', 'w') as fp:\n",
    "            json.dump(everyBalance_dct, fp)\n",
    "        with open(f'./{checkpoint_dir}/e{e}/EveryReward.json', 'w') as fp:\n",
    "            json.dump(everyReward_dct, fp)\n",
    "        with open(f'./{checkpoint_dir}/e{e}/everyInventory.json', 'w') as fp:\n",
    "            json.dump(everyInventory_dct, fp)\n",
    "        with open(f'./{checkpoint_dir}/e{e}/everyLoss.json', 'w') as fp:\n",
    "            json.dump(everyLoss_dct, fp)\n",
    "        # plot\n",
    "        fig.update_layout(showlegend=True, title_text = \"E{3} final profit RL: {0} vs buyhold: {1}, difference = {2}\".format(round(growth[-1],2),round(growth_buyhold[-1],2),round(abs(growth[-1]-growth_buyhold[-1]),2),e))\n",
    "        fig.add_trace(pgo.Scatter(x=np.arange(len(data)), y=data,\n",
    "                            mode='lines',\n",
    "                            name='data'))\n",
    "        fig.add_trace(pgo.Scatter(x=buy_ind, y=data[buy_ind], marker_color = \"green\",\n",
    "                            mode='markers',\n",
    "                            name='buy'))\n",
    "        fig.add_trace(pgo.Scatter(x=sell_ind, y=data[sell_ind], marker_color = \"red\",\n",
    "                            mode='markers',\n",
    "                            name='sell'))\n",
    "        if showfigs:\n",
    "            fig.show(render = \"browser\")\n",
    "        fig.write_html(\"./{0}/results/e{1}_trades.html\".format(checkpoint_dir,e))\n",
    "        fig.data = [] # reset traces\n",
    "        if using_colab:\n",
    "            !zip -r \"$tbzip_folder\" \"$zip_folder\"\n",
    "            print(\"Zip file created, saved next run\")\n",
    "        \n",
    "    if max(e-1,0) % saveIter == 0 and e != 0:\n",
    "        # we skip one iteration as we assume the zip file creation is done now\n",
    "        if using_colab:\n",
    "            #!zip -r /content/run1_colab_r3v2.zip /content/run1_colab\n",
    "            files.download(tbzip_folder)\n",
    "            #print(\"Attempting download, system sleep initiated\")\n",
    "            #time.sleep(24) # Sleep system for x seconds, to give time for download\n",
    "            #print(\"System sleep terminated, continueing...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final save\n",
    "agent.save_models(310)\n",
    "if using_colab:\n",
    "    #!zip -r /content/run1_colab_r3v2.zip /content/run1_colab\n",
    "    !zip -r \"$tbzip_folder\" \"$zip_folder\"\n",
    "    files.download(tbzip_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually save results\n",
    "'''\n",
    "with open(f'./models/{runName}/{runName}_parameters.txt', 'w') as f:\n",
    "    print(allParameters, file=f)\n",
    "\n",
    "# Saving results\n",
    "with open(f'./{checkpoint_dir}/results/total_rewards.npy', 'wb') as f:\n",
    "    np.save(f, np.array(total_reward_list))\n",
    "with open(f'./{checkpoint_dir}/results/EveryProfit_dct.json', 'w') as fp:\n",
    "    json.dump(EveryProfit_dct, fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading results\n",
    "'''\n",
    "with open(f'./{checkpoint_dir}/total_rewards.npy', 'rb') as f:\n",
    "    total_reward_list = np.load(f)\n",
    "with open(f'./{checkpoint_dir}/EveryProfit_dct.json', 'r') as fp:\n",
    "    EveryProfit_dct = json.load(fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup naive benchmark, buy and hold for training data\n",
    "test_data, test_data_extra = getData(\"testdata\", data_extraWindow)\n",
    "data = test_data\n",
    "data_extra = test_data_extra\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(test_data)), y=test_data,\n",
    "                    mode='lines',\n",
    "                    name='stock growth'))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(test_data_extra)), y=test_data_extra,\n",
    "                    mode='lines',\n",
    "                    name='stock MA adjusted W ={}'.format(data_extraWindow)))\n",
    "fig.show()\n",
    "growth_buyhold_per = (data[-1]-data[0])/data[0]\n",
    "\n",
    "print(\"Naive buy & hold strategy on TEST data has a portfolio growth of {0}% per asset bought at the start of 2015 and sold at 2018\".format(round(growth_buyhold_per,3)))\n",
    "growth_buyhold_cash = agent.n_budget*data[0]*growth_buyhold_per\n",
    "growth_buyhold = (agent.n_budget*(data-data[0]))[0:-1]\n",
    "print(\"For current budget of {0}, this means that for {1} stocks bought in 2015 and sold in 2018 one achieves a final portfolio growth of {2}\".format(formatPrice(agent.n_budget*data[0]),agent.n_budget,formatPrice(growth_buyhold_cash)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = \"runs/version3/run1\"\n",
    "agent.load_models(load_dir,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation cell\n",
    "data = test_data # sanity check\n",
    "data_extra = test_data_extra\n",
    "l = len(data) - 1\n",
    "agent.is_eval = True\n",
    "agent.budget = agent.n_budget*data[0]\n",
    "done = False\n",
    "terminate = False\n",
    "\n",
    "\n",
    "# initialize and container setup\n",
    "state = getState(data, 0, window_size + 1)\n",
    "agent.reset()\n",
    "total_reward = 0\n",
    "profits = []\n",
    "balances = []\n",
    "rewards = []\n",
    "inventories = [] # inventory value (only stocks)\n",
    "pbar = tqdm(total=l, position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "\n",
    "for t in range(l):\n",
    "    profit = 0 # initialize\n",
    "    change = 0 \n",
    "    action, action_prob = agent.act(state)\n",
    "    next_state = getState(data, t + 1, window_size + 1)\n",
    "\n",
    "    if action == 1 and agent.balance > data[t]:\n",
    "        # buying stock, only if there is balance though\n",
    "        agent.inventory.append(data[t])\n",
    "        agent.inventory_ma.append(data_extra[t])\n",
    "        change = -data[t]\n",
    "\n",
    "    elif action == 2 and len(agent.inventory) > 0:\n",
    "        # selling stock, only if there are stocks held\n",
    "        '''\n",
    "        bought_price = agent.inventory.pop(0)\n",
    "        '''\n",
    "        _ = agent.inventory.pop(0)\n",
    "        bought_price = agent.inventory_ma.pop(0)\n",
    "        profit = data_extra[t] - bought_price\n",
    "        change = data[t]\n",
    "\n",
    "    '''\n",
    "    else:\n",
    "        # hold stocks\n",
    "        profit = 0 # explicit reset\n",
    "        change = 0\n",
    "        terminate = agent.update_balance(0,inventory_value) # check portfolio based on stock changes\n",
    "    # all statement done in else loop are initialized at start of iteration\n",
    "    '''\n",
    "    agent.update_balance(change)\n",
    "    agent.update_inventory(data[t])\n",
    "    terminate = agent.check_threshold()\n",
    "    if terminate or t == l-1:\n",
    "        # terminating i.e. last iteration \n",
    "        done = True # terminal state has no 'next' so change done variable before use in next lines\n",
    "\n",
    "    reward = rewarder.get_reward(agent, profit, data[t], done)\n",
    "    total_reward += reward\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    # collect \n",
    "    balances.append(agent.balance)\n",
    "    inventories.append(agent.inventory_value)\n",
    "    profits.append(profit)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    if terminate:\n",
    "        # ensure consistent length of arrays then terminate trial\n",
    "        balances = np.pad(balances,(0,l-t-1),'constant',constant_values=(0,balances[-1])).tolist()\n",
    "        inventories = np.pad(inventories,(0,l-t-1),'constant',constant_values=(0,inventories[-1])).tolist()\n",
    "        profits = np.pad(profits,(0,l-t-1),'constant',constant_values=(0,0)).tolist()\n",
    "        rewards = np.pad(rewards,(0,l-t-1),'constant',constant_values=(0,0)).tolist()\n",
    "        print(\"Terminating run at balance violation\")\n",
    "        break\n",
    "        \n",
    "    pbar.set_description(\"Testing of model| portfolio value: {0} | Balance: {1} | total reward: {2}\".format(formatPrice(agent.balance+agent.inventory_value),formatPrice(agent.balance),formatPrice(total_reward)))\n",
    "    pbar.update()\n",
    "        \n",
    "    # collect & save\n",
    "growth = (np.array(balances)+np.array(inventories)-agent.budget).tolist() # \n",
    "compete = (np.array(growth)-growth_buyhold).tolist() # compete vs buyhold\n",
    "    \n",
    "# collect & save\n",
    "save_df = {\"balance\":balances,\"inventory\":inventories,\"profit\":profits,\"reward\":rewards,\"growth\":growth,\"compete\":compete,\"growth_buyhold\":growth_buyhold.tolist()}\n",
    "save_df = pd.DataFrame(save_df).to_csv(\"./{0}/results/res_validation.csv\".format(checkpoint_dir))\n",
    "print(\"succesfully saved csv file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting RL vs buyhold\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(growth)), y=np.array(growth),\n",
    "                    mode='lines',\n",
    "                    name='growth_RL'))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(growth)), y=np.array(growth_buyhold),\n",
    "                    mode='lines',\n",
    "                    name='growth_buyhold'))\n",
    "fig.add_trace(pgo.Scatter(x=buy_ind, y=data[buy_ind], marker_color = \"green\",\n",
    "                    mode='markers',\n",
    "                    name='buy'))\n",
    "fig.add_trace(pgo.Scatter(x=sell_ind, y=data[sell_ind], marker_color = \"red\",\n",
    "                    mode='markers',\n",
    "                    name='sell'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing whether model even learnt\n",
    "model1 = Agent(window_size, batch_size)\n",
    "model1 = model1.critic_target.model\n",
    "a= model1.get_weights()\n",
    "print(a[10])\n",
    "print(\"=================================\")\n",
    "model2 = Agent(window_size, batch_size)\n",
    "model2.critic_target.model.load_weights(\"./run1/e203/critic_local.h5\")\n",
    "b =model2.critic_target.model.get_weights()\n",
    "print(b[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "growth_buyhold[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
