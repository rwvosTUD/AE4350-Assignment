{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE4350 - Bio-inspired Intelligence and Learning Assignment\n",
    "This is the Main notebook for training and testing the DRL portfolio management system\n",
    "<br>Created on Thu May 12 13:22:50 2022\n",
    "<br>@author: Reinier Vos, 4663160-TUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    from utility_v15 import Agent, UtilFuncs, Statistics\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    from AE4350_Assignment.utility_v15 import Agent, UtilFuncs, Statistics\n",
    "except:\n",
    "    pass\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "#In TensorFlow 2.3+, you can disable eager mode anytime using the following method:\n",
    "import tensorflow as tf\n",
    "#tf.autograph.set_verbosity(0)\n",
    "import plotly.graph_objects as pgo\n",
    "import time\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "#tf.config.run_functions_eagerly(False)\n",
    "\n",
    "import logging\n",
    "#logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model summaries\n",
    "from utility_v13 import Actor, Critic\n",
    "window_size = 128\n",
    "\n",
    "convLayers = [[64,4],[64,8],[128,8]]\n",
    "convLayers = [[64,3]]\n",
    "convLayers = []\n",
    "convLayers = [[64,3],[64,3],[128,3],[128,3]]\n",
    "modelsHyper_dct = {\"actor_ts_convLayers\":convLayers, # [filter,kernelsize]\n",
    "                   \"actor_ts_maxPoolsize\":2, # = 0 means not used\n",
    "                   \"actor_ts_maxPoolstrides\":2, # default 1\n",
    "                   \"actor_ts_dLayers\":[256,256,128,64], # agent.LAST layer will be concatted \n",
    "                   \"actor_util_dLayers\":[], #[16,8], # agent.LAST layer will be concatted \n",
    "                   \"actor_comb_dLayers\":[], #[128,64,32],\n",
    "                   \"actor_regularizer\":1e-14, # currently only applied to dense layers\n",
    "                   \"critic_ts_convLayers\":convLayers, # [filter,kernelsize]\n",
    "                   \"critic_ts_maxPoolsize\":0, # = 0 means not used\n",
    "                   \"critic_ts_maxPoolstrides\":2, # default 1\n",
    "                   \"critic_ts_dLayers\":[256,256,128,64], # agent.LAST layer will be concatted \n",
    "                   \"critic_util_dLayers\":[],#[16,8], # agent.LAST layer will be concatted \n",
    "                   \"critic_comb_dLayers\":[], #[128,64,32],\n",
    "                   \"critic_action_dLayers\":[],#[16,8], # action probabilities related layer\n",
    "                   \"critic_final_dLayers\":[32,16], # layers after states and action added \n",
    "                   \"critic_regularizer\":1e-14, # currently only applied to dense layers\n",
    "                   \"use_batchNorm\":False,\n",
    "}\n",
    "\n",
    "actor = Actor(window_size,6,3,modelsHyper_dct)\n",
    "actor.model.summary()\n",
    "critic = Critic(window_size,6,3,modelsHyper_dct)\n",
    "critic.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#inout = tf.keras.layers.Input(shape = (100,), name = \"inout\")\n",
    "class INNER(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(INNER, self).__init__()\n",
    "        self.linear_1 = tf.keras.layers.Dense(32)\n",
    "        self.linear_2 = tf.keras.layers.Dense(32)\n",
    "        self.linear_3 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = self.linear_2(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "inModel = INNER()\n",
    "\n",
    "\n",
    "inner = tf.keras.layers.Input(shape = (100,), name = \"input1\")\n",
    "net = inModel(inner)\n",
    "out = tf.keras.layers.Dense(units = 10)(net)\n",
    "\n",
    "innerModel = tf.keras.models.Model(inputs = inner, outputs = out)\n",
    "innerModel.summary()\n",
    "#output = tf.keras.layers.Dense(units = 3, activation = \"softmax\")(innerModel.output)\n",
    "\n",
    "#outerModel = tf.keras.models.Model(inputs = inout, outputs = output)\n",
    "\n",
    "#outerModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### README\n",
    "<br> Ensure utility v12 is used!\n",
    "<br> This version has is the google colab compatible version, change next cell for desired environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github link\n",
    "\n",
    "using_colab = False\n",
    "pay = False # whether youre using the payed version (i.e. own server)\n",
    "\n",
    "if using_colab:\n",
    "    from google.colab import files\n",
    "    if not pay:\n",
    "        from google.colab import drive\n",
    "        import shutil\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "    !git clone https://github.com/rwvosTUD/AE4350_Assignment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Checkpoint directory already exists and is not empty, please adjust",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 69>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m window_size \u001b[38;5;241m=\u001b[39m agent_dct[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstateTS_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     68\u001b[0m data_extraWindow \u001b[38;5;241m=\u001b[39m  agent_dct[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_extraWindow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 69\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_dct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_dct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_dct\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     71\u001b[0m stats \u001b[38;5;241m=\u001b[39m Statistics(checkpoint_dir, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     72\u001b[0m stats_val \u001b[38;5;241m=\u001b[39m Statistics(checkpoint_dir, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\YEAR2021_2022\\AE MSc C&S\\AE4350-Bio_inspiredIntelligenceAndLearning\\assignment\\scripts\\utility_v15.py:406\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[1;34m(self, modelParams, start_price, checkpoint_dir, rewardParams, extraParams, is_eval)\u001b[0m\n\u001b[0;32m    404\u001b[0m     dirList \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_path)\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dirList) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;66;03m# >2 because results folder & param file (end of __init) is probably created\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint directory already exists and is not empty, please adjust\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_path)\n",
      "\u001b[1;31mException\u001b[0m: Checkpoint directory already exists and is not empty, please adjust"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"runs/v15/v15_tc5_p1\" #v10_reg9_p1\" # \"colabv11_w\"\n",
    "\n",
    "#convLayers = [[32,8],[32,8],[64,4],[64,4],[128,4],[128,4]]\n",
    "#convLayers = [[32,5],[32,5],[64,5],[64,5]]\n",
    "#convLayers = [[64,3],[64,3],[128,3],[128,3]]\n",
    "#convLayers = [[64,4], [64, 4], [128, 4], [128, 4]]\n",
    "#convLayers = [[64,16], [64, 16], [128, 32], [128, 32]]\n",
    "convLayers = []\n",
    "ts_dlayers = [256,256,256,128,64] #[256,256,128,64]\n",
    "comb_layers = [128,128,64,64,32] #[128,64,64,32]\n",
    "critic_final_dLayers = [128,128,64,64,32] #[128,128,64,32]\n",
    "modelsHyper_dct = {\"actor_ts_convLayers\":convLayers, # [filter,kernelsize]\n",
    "                   \"actor_ts_maxPoolsize\":0, # = 0 means not used\n",
    "                   \"actor_ts_maxPoolstrides\":2, # default 1\n",
    "                   \"actor_ts_dLayers\":ts_dlayers, # agent.LAST layer will be concatted \n",
    "                   \"actor_util_dLayers\":[], #[16,8], # agent.LAST layer will be concatted \n",
    "                   \"actor_comb_dLayers\":comb_layers,\n",
    "                   \"actor_regularizer\":1e-14, # currently only applied to dense layers\n",
    "                   \"critic_ts_convLayers\":convLayers, # [filter,kernelsize]\n",
    "                   \"critic_ts_maxPoolsize\":0, # = 0 means not used\n",
    "                   \"critic_ts_maxPoolstrides\":2, # default 1\n",
    "                   \"critic_ts_dLayers\":ts_dlayers, # agent.LAST layer will be concatted \n",
    "                   \"critic_util_dLayers\":[],#[16,8], # agent.LAST layer will be concatted \n",
    "                   \"critic_comb_dLayers\":comb_layers,\n",
    "                   \"critic_action_dLayers\":[],#[16,8], # action probabilities related layer\n",
    "                   \"critic_final_dLayers\":critic_final_dLayers, # layers after states and action added \n",
    "                   \"critic_regularizer\":1e-14, # currently only applied to dense layers\n",
    "                   \"use_batchNorm\":False,\n",
    "}\n",
    "\n",
    "agent_dct = {\"stateTS_size\":64,#128, # i.e. window_size \n",
    "             \"stateUT_size\":6, \n",
    "             \"batch_size\":128,\n",
    "             \"data_extraWindow\":1,\n",
    "             \"n_budget\":1,\n",
    "             \"is_terminal_threshold\":1000,\n",
    "             \"model_hyper\": modelsHyper_dct,\n",
    "             \"tanh_scale\":80,\n",
    "             \"gamma\":0.99,\n",
    "             \"tau\":0.001,\n",
    "             \"episode_window\": 300,\n",
    "             \"mask_input\":False,\n",
    "}\n",
    "\n",
    "reward_dct = {\"rewardType\":7, # NOTICE WHICH REWARD FUNCTION WE USE\n",
    "              \"penalty\":0,\n",
    "              \"hold_scale\": 17,#90, #40,\n",
    "              \"trade_scale\":14,\n",
    "              \"trade_cost\":5,\n",
    "              \"max_holds\":100, # minimum hardcoded to 100!\n",
    "              \"prob_power\":1, \n",
    "}\n",
    "extra_dct = {\"EXTRACASH\" : reward_dct[\"trade_cost\"], # 100\n",
    "            \"EXPAND\" : 5,\n",
    "            \"LAST\" : 5,\n",
    "            \"PROFITDIFF\" : 200,\n",
    "            \"EXPAND_TIMER\":  10,\n",
    "            \"TRADECOST_ACTUAL\" : 3,\n",
    "             \"START_OFFSET\" : 300, #700\n",
    "             \"VALI_EC\":50, # extracash amount for validation\n",
    "}\n",
    "\n",
    "data, data_extra = UtilFuncs.get_data(\"traindata\",agent_dct[\"data_extraWindow\"],\n",
    "                                      colab = using_colab)\n",
    "l = len(data) - 1\n",
    "\n",
    "window_size = agent_dct[\"stateTS_size\"]\n",
    "data_extraWindow =  agent_dct[\"data_extraWindow\"]\n",
    "agent = Agent(agent_dct, data[window_size],\n",
    "              checkpoint_dir, reward_dct, extra_dct) \n",
    "stats = Statistics(checkpoint_dir, training = True)\n",
    "stats_val = Statistics(checkpoint_dir, training = False)\n",
    "print(\"=== ATTENTION: running model for {} stocks ===\".format(agent_dct[\"n_budget\"]))\n",
    "print(l)\n",
    "plt.plot(data[window_size+agent.START_OFFSET:(agent.START_OFFSET+window_size+agent.episode_window)])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loading of models\n",
    "### General loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### loading of model\n",
    "#load_dir = \"runs/run3\"\n",
    "#agent.load_models(load_dir,40)\n",
    "#load_dir = \"runs/version3/run6_r5\"\n",
    "if using_colab:\n",
    "    load_dir = \"AE4350_Assignment/prev_runs\"\n",
    "else:\n",
    "    load_dir = \"runs/colab/content/v15_NomaskA2_p2\" \n",
    "    \n",
    "load_episode = 1500\n",
    "#buffer_episode_size = agent_dct[\"episode_window\"]\n",
    "#agent.load_models(load_dir,load_episode,buffer_episode_size, buffer = True)\n",
    "agent.load_models(load_dir,load_episode, buffer = True, using_colab = using_colab)\n",
    "#agent.memory.memory_counter = load_episode*2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrained actor loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor model loading\n",
    "actor_load_dir = \"actor\"\n",
    "agent.load_models(actor_load_dir,0, critic = False, actor = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_buyhold = UtilFuncs.plot_data(agent, data, data_extra, data_extraWindow, window_size, training = True)\n",
    "data_val, data_extra_val = UtilFuncs.get_data(\"validationdata\", data_extraWindow, colab = using_colab)\n",
    "l_val = len(data_val)-1\n",
    "growth_buyhold_val = UtilFuncs.plot_data(agent, data_val, data_extra_val, data_extraWindow, window_size, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from utility_v12 import UtilFuncs\n",
    "show_figs = False\n",
    "debug = False\n",
    "\n",
    "testing = False\n",
    "#''' # uncomment this when resetting a trial \n",
    "reward_lst = []\n",
    "profitdiff_lst = []\n",
    "expansions_lst = []\n",
    "expand_i = 1\n",
    "\n",
    "history = {}\n",
    "history[\"training_profit\"] = []\n",
    "history[\"training_pratio\"]  = []\n",
    "history[\"validation_profit\"] = []\n",
    "history[\"validation_pratio\"] = []\n",
    "history[\"validation_extraCash\"] = []\n",
    "#'''\n",
    "deadlock_on = False\n",
    "timer = 0 \n",
    "expansion_size = 1 # start expansion_size at 1, will be set to EXPANSION after first\n",
    "\n",
    "saveIter = 100\n",
    "start = 0\n",
    "episode_count = 10000 #300\n",
    "\n",
    "\n",
    "use_terminateFunc = True\n",
    "terminateFunc_on = False\n",
    "#'''\n",
    "deadlock_probStart = 1/10 #1/6  # exploratory probability hack for actions 1 & 2\n",
    "decay = 0.90\n",
    "decay_op = 2-decay\n",
    "deadlock_prob = deadlock_probStart*(decay**(start)) # if we start later in an epoch\n",
    "switch_episode = 50\n",
    "#'''\n",
    "#extra_explore = False # whether to override the original exploration sheme\n",
    "#scaling = 0.1 #0.02\n",
    "tbzip_folder=\"/content/{0}_z.zip\".format(checkpoint_dir)\n",
    "zip_folder = \"/content/{0}\".format(checkpoint_dir)\n",
    "\n",
    "min_t = window_size \n",
    "stats.reset_all(agent.n_budget*data[window_size], growth_buyhold)\n",
    "stats_val.reset_all(agent.n_budget*data_val[window_size], growth_buyhold_val)\n",
    "print(\"\\n ATTENTION DEADLOCK IS SET TO {0} \\n\".format(deadlock_on))\n",
    "prev_terminate = True\n",
    "for e in range(start,episode_count):\n",
    "    agent.is_eval = False # training!\n",
    "    \n",
    "    if e % saveIter == 0 and e != 0:\n",
    "        episode_start = window_size\n",
    "        episode_end = l\n",
    "        agent.is_eval = True # training!\n",
    "    else:\n",
    "        ''' \n",
    "        SHUFFLING START \n",
    "        ''' \n",
    "        \n",
    "        \n",
    "        #deadlock_go = False\n",
    "        if (e-start) > agent.LAST:\n",
    "            profitdiff_mean = np.mean(profitdiff_lst[-agent.LAST:])\n",
    "\n",
    "            if profitdiff_mean <= 0:\n",
    "                # EXPONENTIALLY GROW\n",
    "                deadlock_prob = min(deadlock_prob*decay_op,deadlock_probStart) \n",
    "                timer = 0 # reset\n",
    "            elif profitdiff_mean > agent.PROFITDIFF:\n",
    "                # DECAY\n",
    "                deadlock_prob = max(deadlock_prob*decay,0.0001) \n",
    "                timer += 1\n",
    "            else: \n",
    "                timer = 0  # reset \n",
    "                \n",
    "            print(\"E{0} - Deadlock probability {1} | Timer = {2} | expansions = {3} | Prev mean = {4}\".format(e,\n",
    "                                                                         round(deadlock_prob,3), \n",
    "                                                                         timer,\n",
    "                                                                         expand_i-1,\n",
    "                                                                         profitdiff_mean))\n",
    "            if timer >= agent.EXPAND_TIMER: \n",
    "                expand_i += 1 \n",
    "                timer = 0\n",
    "                expansion_size = agent.EXPAND\n",
    "                #deadlock_go = True\n",
    "\n",
    "        \n",
    "        utils_start = [l,agent.START_OFFSET, agent.episode_window]\n",
    "        episode_start  = UtilFuncs.get_episodeStart(agent, expand_i, expansion_size, utils_start)\n",
    "        episode_end = episode_start+agent.episode_window\n",
    "        print(\"E{2} - Current window [{0},{1}]\".format(episode_start,episode_end,e))\n",
    "    \n",
    "    '''\n",
    "    DEADLOCK SWITCH OFF\n",
    "    \n",
    "    '''\n",
    "    if e == 30:\n",
    "        deadlock_on = False \n",
    "        print(\"\\n ATTENTION DEADLOCK IS SET TO {0} \\n\".format(deadlock_on))\n",
    "    \n",
    "    \n",
    "    #if e >= switch_episode:\n",
    "    pbar = tqdm(total=(episode_end-episode_start), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "    #print(\"== Episode \" + str(e) + \"/\" + str(episode_count) +\" ==\")\n",
    "    \n",
    "    agent.reset(data[episode_start])\n",
    "    agent.balance += agent.EXTRACASH\n",
    "    stats.reset_episode()\n",
    "    stats.extraCash += agent.EXTRACASH \n",
    "    '''\n",
    "    extra cash!\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    sold_price = 0\n",
    "    bought_price = agent.inventory_value\n",
    "    utils_state = [episode_end, stats.n_holds,stats.n_trades, agent.trade_cost]\n",
    "    state = UtilFuncs.get_state(agent, data, episode_start, window_size + 1, utils_state)\n",
    "    \n",
    "    done = False\n",
    "    terminate = False\n",
    "    \n",
    "    #REWARD TYPE SWITCH EXPERIMENT\n",
    "    #agent.switch_rewardType(7,switch_episode,e)\n",
    "    '''\n",
    "    if not e == start: # not prev_terminate and not e == start:\n",
    "        if deadlock_on:\n",
    "            deadlock_prob = deadlock_prob*decay # remove\n",
    "            print(\"E{0} - Deadlock probability {1}\".format(e,prob))\n",
    "    '''\n",
    "    # =============================== ITERATION ======================================================\n",
    "    for t in range(episode_start,episode_end):\n",
    "\n",
    "        utils_act = [deadlock_prob,data[t],agent.trade_cost]\n",
    "        action, action_prob = agent.act(state, utils_act)\n",
    "        \n",
    "        # DEADLOCK EXPERIMENT\n",
    "        action = UtilFuncs.break_deadlock(agent,action,e,utils_act, on = deadlock_on) \n",
    "            \n",
    "        # Hanle action\n",
    "        flags = [use_terminateFunc, terminateFunc_on]\n",
    "        utils_hdlAct = [action_prob]\n",
    "        action, profit,  impossible, terminate, term_msg = UtilFuncs.handle_action(agent, stats, action, data, \n",
    "                                                                                   t, flags, utils_hdlAct, training = True)\n",
    "        \n",
    "        # terminate if required\n",
    "        if terminate or t == episode_end-1:\n",
    "            # terminating i.e. agent.LAST iteration \n",
    "            done = True # terminal state has no 'next' so change done variable before use in next lines\n",
    "            \n",
    "        # get reward\n",
    "        if t < data_extraWindow:\n",
    "            ptn = data_extra[t]\n",
    "        else: \n",
    "            ptn = data[t-data_extraWindow]\n",
    "        utils_reward = [data[t],data[t-1], data[t+1], action, action_prob[0], stats.n_trades, stats.n_holds, impossible, l, terminate]\n",
    "        reward = agent.get_reward(agent, profit, utils_reward, done)\n",
    "        stats.total_reward += reward\n",
    "        \n",
    "        # take step\n",
    "        utils_state = [episode_end, stats.n_holds,stats.n_trades, agent.trade_cost]\n",
    "        next_state = UtilFuncs.get_state(agent, data, t + 1, window_size + 1, utils_state)\n",
    "        if e % saveIter != 0 or e == 0: \n",
    "            '''\n",
    "            SMALL WINDOW EXPERIMENT\n",
    "            '''\n",
    "            actor_local_loss = agent.step(action_prob, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if terminate:\n",
    "            # ensure consistent length of arrays then terminate trial\n",
    "            if t >= min_t:\n",
    "                utils_pad = [l,t]\n",
    "                stats.pad_on_terminate(utils_pad)    \n",
    "            print(\"Episode {0} was terminated at {1}/{2} due to {3}\".format(e,t-window_size,episode_end-episode_start, term_msg))\n",
    "            break\n",
    "            \n",
    "        # collect \n",
    "        utils_saveIter = [profit, reward, actor_local_loss, action, t-episode_start]\n",
    "        stats.collect_iteration(agent,utils_saveIter)\n",
    "        \n",
    "        pbar.set_description(\"Episode {0}| Portfolio: {1}| Balance: {2}| Inventory: {3}| RewardAcc: {4}\".format(int(e),\n",
    "                                                                                                                UtilFuncs.to_currency(agent.balance+agent.inventory_value),\n",
    "                                                                                                                UtilFuncs.to_currency(agent.balance),\n",
    "                                                                                                                UtilFuncs.to_currency(agent.inventory_value),\n",
    "                                                                                                                UtilFuncs.to_currency(stats.total_reward)))\n",
    "        pbar.update()\n",
    "        \n",
    "    # ============================ END ITERATION =================================================\n",
    "    print(\"E{3} - Amount of +trades {0}, all trades {1} | ratio = {2}\".format(stats.n_posiProfits, \n",
    "                                                                              stats.n_trades, \n",
    "                                                                              round(stats.n_posiProfits/max(1,stats.n_trades),3), \n",
    "                                                                              e))\n",
    "    '''\n",
    "    SMALL WINDOW EXPERIMENT\n",
    "\n",
    "    BREAK CURRENT EPISODE, WE WONT SAVE ANYWAY\n",
    "    '''\n",
    "    profitBuyhold = data[episode_end-1]-data[episode_start]\n",
    "    #profitRL = agent.balance+agent.inventory_value-data[episode_start]-agent.trade_cost*stats.n_trades-stats.extraCash\n",
    "    profitRL = agent.balance+agent.inventory_value-data[episode_start]-agent.TRADECOST_ACTUAL*stats.n_trades-stats.extraCash\n",
    "    profitdiff = profitRL-profitBuyhold\n",
    "    print(\"E{3} - RL profit = {0} | Buyhold = {1} | diff = {2} | tradecost = {4}\".format(round(profitRL,2),\n",
    "                                                                round(profitBuyhold,2),\n",
    "                                                                round(profitdiff,2),\n",
    "                                                                e,\n",
    "                                                                #agent.trade_cost*stats.n_trades,\n",
    "                                                                agent.TRADECOST_ACTUAL*stats.n_trades,\n",
    "                                                                ))\n",
    "    print(\"E{0} - impossibles {1}/{2} = {3}\".format(e, stats.n_impossible,\n",
    "                                             stats.n_1or2,\n",
    "                                             round(stats.n_impossible/stats.n_1or2,3)))\n",
    "    reward_lst.append(stats.total_reward)\n",
    "    profitdiff_lst.append(profitdiff) \n",
    "    expansions_lst.append(expand_i)\n",
    "    '''\n",
    "    if e % 25 == 0 and e != 0:\n",
    "        x = np.arange(start,e+1)\n",
    "        y = np.array(reward_lst)\n",
    "        coef = np.polyfit(x,y,1)\n",
    "        plt.plot(x,y)\n",
    "        plt.plot(x,x*coef[0]+coef[1])\n",
    "        plt.plot(x,np.zeros((e-start+1)))\n",
    "    '''\n",
    "    if max(e-1,0) % saveIter == 0 and e != 0 and e != 1 and not debug:\n",
    "        if not prev_terminate:\n",
    "            # we skip one iteration as we assume the zip file creation is done now\n",
    "            if using_colab and not pay: \n",
    "                #!zip -r /content/run1_colab_r3v2.zip /content/run1_colab\n",
    "                #files.download(tbzip_folder)\n",
    "                shutil.copy(tbzip_folder, \"/content/drive/MyDrive\")\n",
    "                #b = 1\n",
    "                #print(\"Attempting download, system sleep initiated\")\n",
    "                #time.sleep(24) # Sleep system for x seconds, to give time for download\n",
    "                #print(\"System sleep terminated, continueing...\")\n",
    "                \n",
    "    if e % saveIter != 0 or e == 0: \n",
    "        continue\n",
    "        \n",
    "\n",
    "    \n",
    "    if True: #not terminate:\n",
    "        stats.collect_episode(agent,e, [])\n",
    "        history[\"training_profit\"].append(stats.compete[-1])\n",
    "        history[\"training_pratio\"].append(stats.n_posiProfits/max(1,stats.n_trades))\n",
    "        \n",
    "    if e % saveIter == 0 and e != 0: #and not terminate:\n",
    "        if not debug:\n",
    "            # plot and save\n",
    "            agent.save_models(e)\n",
    "            utils_fig = [l, window_size]\n",
    "            stats.plot_figure(data, e, utils_fig, show_figs = show_figs)\n",
    "            stats.save_statistics(e)\n",
    "            extraHistory = {}\n",
    "            extraHistory[\"reward_lst\"] = reward_lst\n",
    "            extraHistory[\"profitdiff_lst\"] = profitdiff_lst\n",
    "            extraHistory[\"expansions_lst\"] = expansions_lst \n",
    "\n",
    "            with open(f'./{checkpoint_dir}/EXTRAhistory.json', 'w') as fp:\n",
    "                json.dump(extraHistory, fp)\n",
    "    \n",
    "            # ================ VALIDATION LOOP ===============================\n",
    "            stats_val.reset_episode()\n",
    "            stats_val.extraCash += agent.VALI_EC\n",
    "            agent.is_eval = True\n",
    "            agent.reset(data_val[window_size])\n",
    "            agent.balance += agent.VALI_EC\n",
    "            for t in trange(window_size,l_val):\n",
    "                utils_state = [l_val, stats_val.n_holds,stats_val.n_trades, agent.trade_cost]\n",
    "                state = UtilFuncs.get_state(agent, data_val, t, window_size + 1, utils_state)\n",
    "                utils_act = [deadlock_prob,data_val[t]]\n",
    "                action, action_prob = agent.act(state, utils_act)\n",
    "\n",
    "                flags = [use_terminateFunc, terminateFunc_on]\n",
    "                utils_hdlAct = [action_prob]\n",
    "                action, profit,  impossible, _, _ = UtilFuncs.handle_action(agent, stats_val, action, data_val, \n",
    "                                                                                           t, flags, utils_hdlAct, training = True)\n",
    "                utils_saveIter = [profit, 0., 0., action, t-window_size]\n",
    "                stats_val.collect_iteration(agent,utils_saveIter)\n",
    "\n",
    "            stats_val.collect_episode(agent, e, [])\n",
    "            print(\"Final validation profit = {0} | extra cash = {1} | +/all trades= {2}/{3}\".format(round(stats_val.compete[-1],2),\n",
    "                                                                            round(stats_val.extraCash,2),\n",
    "                                                                              stats_val.n_posiProfits,\n",
    "                                                                               stats_val.n_trades,))\n",
    "            utils_fig = [l_val, window_size]\n",
    "            stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)\n",
    "            history[\"validation_profit\"].append(stats_val.compete[-1])\n",
    "            history[\"validation_pratio\"].append(stats_val.n_posiProfits/max(1,stats_val.n_trades))\n",
    "            history[\"validation_extraCash\"].append(stats_val.extraCash)\n",
    "            # ============= END VALIDATION LOOP =============================\n",
    "            with open(f'./{checkpoint_dir}/e{e}/history.json', 'w') as fp:\n",
    "                json.dump(history, fp)\n",
    "            \n",
    "            \n",
    "        if using_colab and not debug:\n",
    "            !zip -r \"$tbzip_folder\" \"$zip_folder\"\n",
    "            print(\"Zip file created, saved next run\")\n",
    "        \n",
    "\n",
    "    '''\n",
    "    EXPERIMENT\n",
    "    '''\n",
    "    if terminate:\n",
    "        prev_terminate = True\n",
    "    else:\n",
    "        prev_terminate = False\n",
    "            \n",
    "    #if debug and not terminate:\n",
    "    #    print(\"E{3} - Amount of impossibles {0} out of trades {1}, ratio = {2}\".format(n_impossible, n_1or2, n_impossible/n_1or2, e))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually saving replay buffer\n",
    "np.savez_compressed('/content/{0}/buffer_e{1}.npz'.format(checkpoint_dir, e), \n",
    "                    a = agent.memory.memory_state,\n",
    "                    b = agent.memory.memory_nextState, \n",
    "                    c = agent.memory.memory_action,\n",
    "                    d = agent.memory.memory_reward,\n",
    "                    e = agent.memory.memory_dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reward_lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraHistory = {}\n",
    "extraHistory[\"reward_lst\"] = reward_lst\n",
    "extraHistory[\"agent.PROFITDIFF_lst\"] = agent.PROFITDIFF_lst\n",
    "\n",
    "with open(f'./{checkpoint_dir}/EXTRAhistory.json', 'w') as fp:\n",
    "    json.dump(extraHistory, fp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(agent.PROFITDIFF_lst)\n",
    "plt.plot(reward_lst[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dir =  \"runs/colab/content/colabv10_w150\"\n",
    "load_dir =  \"runs/colab/content/v10_reg9_p1\"\n",
    "files = os.listdir(load_dir)\n",
    "#maxEps = max([list(map(int, re.findall('\\d+', file))) for file in files])[0]\n",
    "maxEps = 159 #90\n",
    "with open(f'./{load_dir}/e{maxEps}/history.json', 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history['training'])\n",
    "plt.figure()\n",
    "plt.plot(history['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = {}\n",
    "history[\"validation\"] = []\n",
    "# ================ VALIDATION LOOP ===============================\n",
    "load_dir =  \"runs/colab/content/v10_test_p2\"\n",
    "files = os.listdir(load_dir)\n",
    "files = [list(map(int, re.findall('\\d+', file))) for file in files]\n",
    "files.sort()\n",
    "\n",
    "files = files[2:] # skip first two\n",
    "\n",
    "'''\n",
    "RESET THE SLICING TO [2:] NOT [3:]\n",
    "\n",
    "'''\n",
    "episodes = [x[0] for x in files]\n",
    "\n",
    "agent.EXTRACASH = 0\n",
    "HORIZON = 0\n",
    "\n",
    "#episodes = episodes[-1:]\n",
    "\n",
    "for e in episodes:\n",
    "    agent.load_models(load_dir,e)\n",
    "    stats_val.reset_episode()\n",
    "    agent.is_eval = True\n",
    "    agent.reset(data_val[window_size])\n",
    "    \n",
    "    #agent.balance = agent.EXTRACASH\n",
    "    #stats_val.agent.EXTRACASH = agent.EXTRACASH\n",
    "    \n",
    "    for t in trange(window_size,l_val):\n",
    "        utils_state = [l_val, stats_val.n_holds,stats_val.n_trades, agent.trade_cost]\n",
    "        state = UtilFuncs.get_state(agent, data_val, t, window_size + 1, utils_state)\n",
    "        utils_act = [prob,data_val[t]]\n",
    "        action, action_prob = agent.act(state, utils_act)\n",
    "\n",
    "        flags = [use_terminateFunc, terminateFunc_on]\n",
    "        utils_hdlAct = [action_prob]\n",
    "        action, profit,  impossible, terminate, term_msg = UtilFuncs.handle_action(agent, stats_val, action, data_val, \n",
    "                                                                                   t, flags, utils_hdlAct, training = False)\n",
    "        utils_saveIter = [profit, 0., 0., action, t-window_size]\n",
    "        stats_val.collect_iteration(agent,utils_saveIter)\n",
    "        \n",
    "        #if action == 2 and agent.balance < np.min(data_val[(t+1):(t+HORIZON)]):\n",
    "        '''\n",
    "        if not bool(agent.inventory) and agent.balance < np.min(data_val[(t+1):(t+HORIZON)]): \n",
    "            #print(t)\n",
    "            #print(\"yes\")\n",
    "            n_mins = sum(data_val[(t+1):] < agent.balance) # amount of times there is an opportunity to return to market\n",
    "            if n_mins <= 5:\n",
    "                stats_val.agent.EXTRACASH += data_val[t+1] - agent.balance # extra cash required for purchase\n",
    "                stats_val.xtr_ind.append(t+1)\n",
    "                agent.reset(data_val[t+1]) # reset the portfolio\n",
    "        '''\n",
    "        \n",
    "        if not bool(agent.inventory) and agent.balance < data_val[t] and np.argmax(action_prob) == 1:\n",
    "        #if not bool(agent.inventory) and np.argmax(action_prob) == 1 and all(agent.balance < data_val[(t):(t+HORIZON)]):\n",
    "            #agent.EXTRACASHreq = data_val[t+1] - agent.balance\n",
    "            stats_val.agent.EXTRACASH += data_val[t] - agent.balance # extra cash required for purchase\n",
    "            stats_val.xtr_ind.append(t)\n",
    "            agent.reset(data_val[t]) # reset the portfolio, as if we did buy it\n",
    "            #agent.balance += agent.EXTRACASHreq\n",
    "\n",
    "        #if t < 490 and t > 475:\n",
    "        #    print(\"a={0}/b={1}/t={2}\".format(action, agent.balance, t))\n",
    "\n",
    "    stats_val.collect_episode(agent, e, [])\n",
    "    print(\"Final validation profit = {0} & extra cash = {1}\".format(round(stats_val.compete[-1],2), \n",
    "                                                                    round(stats_val.agent.EXTRACASH,2)))\n",
    "    utils_fig = [l_val, window_size]\n",
    "    stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)\n",
    "    history[\"validation\"].append(stats_val.compete[-1])\n",
    "    # ============= END VALIDATION LOOP =============================\n",
    "    print(\"done with episode {}\".format(e))\n",
    "with open(f'./{load_dir}/results/validation_redone.json', 'w') as fp:\n",
    "    json.dump(history, fp)\n",
    "    \n",
    "    \n",
    "# PLOTTING\n",
    "x = np.arange(0,len(episodes))\n",
    "y= np.array(history[\"validation\"])\n",
    "coef = np.polyfit(x,y,1)\n",
    "plt.plot(np.array(episodes),np.array(history[\"validation\"]))\n",
    "plt.plot(np.array(episodes),np.arange(len(episodes))*coef[0]+coef[1])\n",
    "plt.plot(np.array(episodes),np.zeros((len(episodes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#episodes = episodes[-10:]\n",
    "x = np.arange(0,len(episodes))\n",
    "y= np.array(history[\"validation\"])\n",
    "coef = np.polyfit(x,y,1)\n",
    "plt.plot(np.array(episodes),np.array(history[\"validation\"]))\n",
    "plt.plot(np.array(episodes),np.arange(len(episodes))*coef[0]+coef[1])\n",
    "plt.plot(np.array(episodes),np.zeros((len(episodes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(episodes),np.array(history[\"validation\"][82:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_val.reset_episode()\n",
    "agent.is_eval = True\n",
    "agent.reset(data_val[window_size])\n",
    "for t in trange(window_size,l_val):\n",
    "    utils_state = [l_val, stats_val.n_holds,stats_val.n_trades]\n",
    "    state = UtilFuncs.get_state(agent, data_val, t, window_size + 1, utils_state)\n",
    "    utils_act = [prob,data_val[t]]\n",
    "    action, action_prob = agent.act(state, utils_act)\n",
    "\n",
    "    flags = [use_terminateFunc, terminateFunc_on]\n",
    "    action, profit,  impossible, terminate, term_msg = UtilFuncs.handle_action(agent, stats_val, action, data_val, \n",
    "                                                                               t, flags, training = False)\n",
    "    utils_saveIter = [profit, 0., 0., action]\n",
    "    stats_val.collect_iteration(agent,utils_saveIter)\n",
    "\n",
    "stats_val.collect_episode(agent, e, [])\n",
    "utils_fig = [l_val, window_size]\n",
    "stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = agent.memory\n",
    "with open(f'./memoryBIG.npy', 'wb') as f:\n",
    "    np.save(f, dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats_val.collect_episode(agent, e, [])\n",
    "utils_fig = [l_val, window_size]\n",
    "stats_val.plot_figure(data_val, e, utils_fig, show_figs = show_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final save\n",
    "agent.save_models(0)\n",
    "if using_colab:\n",
    "    #!zip -r /content/run1_colab_r3v2.zip /content/run1_colab\n",
    "    !zip -r \"$tbzip_folder\" \"$zip_folder\"\n",
    "    files.download(tbzip_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually save results\n",
    "'''\n",
    "with open(f'./models/{runName}/{runName}_parameters.txt', 'w') as f:\n",
    "    print(allParameters, file=f)\n",
    "\n",
    "# Saving results\n",
    "with open(f'./{checkpoint_dir}/results/total_rewards.npy', 'wb') as f:\n",
    "    np.save(f, np.array(total_reward_list))\n",
    "with open(f'./{checkpoint_dir}/results/EveryProfit_dct.json', 'w') as fp:\n",
    "    json.dump(EveryProfit_dct, fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading results\n",
    "'''\n",
    "with open(f'./{checkpoint_dir}/total_rewards.npy', 'rb') as f:\n",
    "    total_reward_list = np.load(f)\n",
    "with open(f'./{checkpoint_dir}/EveryProfit_dct.json', 'r') as fp:\n",
    "    EveryProfit_dct = json.load(fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup naive benchmark, buy and hold for training data\n",
    "test_data, test_data_extra = UtilFuncs.get_data(\"testdata\", data_extraWindow)\n",
    "data = test_data\n",
    "data_extra = test_data_extra\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(test_data)), y=test_data,\n",
    "                    mode='lines',\n",
    "                    name='stock growth'))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(test_data_extra)), y=test_data_extra,\n",
    "                    mode='lines',\n",
    "                    name='stock MA adjusted W ={}'.format(data_extraWindow)))\n",
    "fig.show()\n",
    "growth_buyhold_per = (data[-1]-data[window_size])/data[window_size]\n",
    "\n",
    "print(\"Naive buy & hold strategy on TEST data has a portfolio growth of {0}% per asset bought at the start of 2015 and sold at 2018\".format(round(growth_buyhold_per,3)))\n",
    "growth_buyhold_cash = agent.n_budget*data[window_size]*growth_buyhold_per\n",
    "growth_buyhold = (agent.n_budget*(data-data[window_size]))[window_size:-1]\n",
    "print(\"For current budget of {0}, this means that for {1} stocks bought in 2015 and sold in 2018 one achieves a final portfolio growth of {2}\".format(UtilFuncs.to_currency(agent.n_budget*data[window_size]),agent.n_budget,UtilFuncs.to_currency(growth_buyhold_cash)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dir = \"runs/colab/content/run1_colab_noPTcont\"\n",
    "load_dir = \"runs/colab/content/run1_v6_prftAdj+holdbonus25\"\n",
    "try:\n",
    "    agent.setup_validation(load_dir)\n",
    "except:\n",
    "    pass\n",
    "checkpoint_dir = load_dir\n",
    "val_episode = 20\n",
    "agent.load_models(load_dir,val_episode)\n",
    "agent.load_attributes(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation cell\n",
    "data = test_data # sanity check\n",
    "data_extra = test_data_extra\n",
    "l = len(data) - 1\n",
    "l1 = 2263\n",
    "agent.is_eval = True #True\n",
    "agent.EXTRACASH = 0\n",
    "showfigs = True\n",
    "deadlock_on = False # must remain off!\n",
    "debug = False\n",
    "use_terminateFunc = True\n",
    "terminateFunc_on = False\n",
    "prob = 0 # not used\n",
    "# initialize and container setup\n",
    "agent.reset(data[window_size])\n",
    "agent.validation_agent.EXTRACASH(agent.EXTRACASH) # function must be after reset!\n",
    "total_reward = 0 # total profit resets every epsiode \n",
    "n_trades = 0 \n",
    "n_impossible = 0 \n",
    "n_holds = 0 \n",
    "n_1or2 = 1 # 1 not zero because we cant have division by zero \n",
    "profits = []\n",
    "balances = []\n",
    "rewards = []\n",
    "inventories = [] # inventory value (only stocks)\n",
    "actor_local_losses = []\n",
    "buy_ind = []\n",
    "sell_ind = []\n",
    "utils_state = [l1, n_holds, n_trades]\n",
    "state = UtilFuncs.get_state(agent, data, window_size, window_size + 1, utils_state)\n",
    "pt1 = data_extra[-1]\n",
    "\n",
    "pbar = tqdm(total=l, position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "done = False\n",
    "terminate = False\n",
    "\n",
    "for t in range(window_size,l):\n",
    "    profit = 0 # initialize\n",
    "    change = 0 \n",
    "    impossible = False\n",
    "    utils_act = [prob,data[t]]\n",
    "    action, action_prob = agent.act(state, utils_act)\n",
    "\n",
    "    # DEADLOCK EXPERIMENT\n",
    "    #action = UtilFuncs.break_deadlock(agent,action,e,utils_act, on = deadlock_on) \n",
    "\n",
    "    # =========================== ACTION SPACE ======================================\n",
    "    if action == 0:\n",
    "        n_holds += 1\n",
    "\n",
    "    elif action == 1:\n",
    "        n_1or2 += 1\n",
    "        \n",
    "        if len(agent.inventory) == 0: #max one stock \n",
    "            if agent.balance > data[t]:\n",
    "                # BUYING stock, only if there is balance though\n",
    "                agent.inventory.append(data[t])\n",
    "                sold_price = agent.inventory_conj.pop(0)\n",
    "\n",
    "                profit = sold_price - data[t]\n",
    "\n",
    "                change = -data[t]\n",
    "                buy_ind.append(t)\n",
    "                n_trades += 1\n",
    "                n_holds = 0 # reset counter\n",
    "            else: \n",
    "                agent.EXTRACASH += data[t] - agent.balance\n",
    "                agent.reset(data[t])\n",
    "        else:\n",
    "            impossible = True\n",
    "            n_impossible += 1\n",
    "            n_holds += 1 # effectively no buy is a hold\n",
    "            if not use_terminateFunc:\n",
    "                terminate = True\n",
    "\n",
    "    elif action == 2:\n",
    "        n_1or2 += 1\n",
    "        if len(agent.inventory) > 0: \n",
    "            # SELLING stock, only if there are stocks held\n",
    "\n",
    "            bought_price = agent.inventory.pop(0)\n",
    "            agent.inventory_conj.append(data[t])\n",
    "\n",
    "            profit = data[t] - bought_price \n",
    "\n",
    "            change = data[t]\n",
    "            sell_ind.append(t)\n",
    "            n_trades += 1\n",
    "            n_holds = 0 # reset counter\n",
    "        else:\n",
    "            impossible = True\n",
    "            n_impossible += 1\n",
    "            n_holds += 1 # effectively no sell is a hold\n",
    "            if not use_terminateFunc:\n",
    "                terminate = True\n",
    "    # =============================== END ======================================\n",
    "\n",
    "    agent.update_balance(change)\n",
    "    agent.update_inventory(data[t])\n",
    "    if use_terminateFunc:\n",
    "        utils_term = [n_impossible, np.min(data[(t+1):])]\n",
    "        terminate = agent.check_threshold(utils_term, terminateFunc_on= terminateFunc_on)\n",
    "        #terminate = agent.check_threshold()\n",
    "    if terminate or t == l-1:\n",
    "        # terminating i.e. agent.LAST iteration \n",
    "        done = True # terminal state has no 'next' so change done variable before use in next lines\n",
    "\n",
    "\n",
    "    if t < data_extraWindow:\n",
    "        ptn = data_extra[t]\n",
    "    else: \n",
    "        ptn = data[t-data_extraWindow]\n",
    "    utils_reward = [data[t],pt1, ptn, action, n_trades, n_holds, impossible, l, terminate]\n",
    "    pt1 = data[t]\n",
    "    '''\n",
    "    change names of variables if it works\n",
    "    '''\n",
    "    reward = agent.get_reward(agent, profit, utils_reward, done)\n",
    "    total_reward += reward\n",
    "\n",
    "    utils_state = [l, n_holds,n_trades]\n",
    "    next_state = UtilFuncs.get_state(agent, data, t + 1, window_size + 1, utils_state)\n",
    "\n",
    "    #actor_local_loss = agent.step(action_prob, reward, next_state, done)\n",
    "    state = next_state\n",
    "\n",
    "    # collect \n",
    "    balances.append(agent.balance)\n",
    "    inventories.append(agent.inventory_value)\n",
    "    profits.append(profit)\n",
    "    rewards.append(reward)\n",
    "    #actor_local_losses.append(float(actor_local_loss))\n",
    "    \n",
    "    if terminate:\n",
    "        # ensure consistent length of arrays then terminate trial\n",
    "        if t >= 1500:\n",
    "            balances = np.pad(balances,(0,l-t-1),'constant',constant_values=(0,balances[-1])).tolist()\n",
    "            inventories = np.pad(inventories,(0,l-t-1),'constant',constant_values=(0,inventories[-1])).tolist()\n",
    "            profits = np.pad(profits,(0,l-t-1),'constant',constant_values=(0,0)).tolist()\n",
    "            rewards = np.pad(rewards,(0,l-t-1),'constant',constant_values=(0,0)).tolist()\n",
    "            #actor_local_losses = np.pad(actor_local_losses,(0,l-t-1),'constant',constant_values=(0,actor_local_losses[-1])).tolist()\n",
    "        print(\"Episode {0} was terminated at {1}/{2} due to terminal state\".format('test',t,l))\n",
    "        break\n",
    "\n",
    "    pbar.set_description(\"VALIDATION e{4}| Portfolio: {0}| Balance: {1}| Inventory: {2}| RewardAcc: {3}\".format(UtilFuncs.to_currency(agent.balance+agent.inventory_value),\n",
    "                                                                                                            UtilFuncs.to_currency(agent.balance),\n",
    "                                                                                                            UtilFuncs.to_currency(agent.inventory_value),\n",
    "                                                                                                            UtilFuncs.to_currency(total_reward),\n",
    "                                                                                                            val_episode))\n",
    "    pbar.update()\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    # collect & save\n",
    "growth = (np.array(balances)+np.array(inventories)-agent.budget).tolist() # \n",
    "compete = (np.array(growth)-growth_buyhold).tolist() # compete vs buyhold\n",
    "print(\"Amount of impossibles {0}, trades {1} (+1!)| ratio = {2}\".format(n_impossible, n_1or2, n_impossible/n_1or2))\n",
    "fig.update_layout(showlegend=True, xaxis_range=[window_size, l], \n",
    "                  title_text = \"Test e{5} | profit RL: {0} vs buyhold: {1}, difference = {2} | extra cash = {3}| Impossible ratio {4}\".format(round(growth[-1],2),\n",
    "                                                                                                                     round(growth_buyhold[-1],2),\n",
    "                                                                                                                     round(growth[-1]-growth_buyhold[-1],2),\n",
    "                                                                                                                     agent.EXTRACASH,\n",
    "                                                                                                                     round(n_impossible/n_1or2,3),\n",
    "                                                                                                                     val_episode))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(data)), y=data,\n",
    "                    mode='lines',\n",
    "                    name='data'))\n",
    "fig.add_trace(pgo.Scatter(x=buy_ind, y=data[buy_ind], marker_color = \"green\",\n",
    "                    mode='markers',\n",
    "                    name='buy'))\n",
    "fig.add_trace(pgo.Scatter(x=sell_ind, y=data[sell_ind], marker_color = \"red\",\n",
    "                    mode='markers',\n",
    "                    name='sell'))\n",
    "if showfigs:\n",
    "    fig.show(render = \"browser\")\n",
    "fig.write_html(\"./{0}/validation/e{1}_tradesTest.html\".format(checkpoint_dir,val_episode))\n",
    "    \n",
    "# collect & save\n",
    "save_df = {\"balance\":balances,\"inventory\":inventories,\"profit\":profits,\"reward\":rewards,\"growth\":growth,\"compete\":compete,\"growth_buyhold\":growth_buyhold.tolist()}\n",
    "save_df = pd.DataFrame(save_df).to_csv(\"./{0}/validation/e{1}_testResults.csv\".format(checkpoint_dir,val_episode))\n",
    "print(\"Succesfully saved csv file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.EXTRACASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_state = [l, n_holds]\n",
    "t = 99\n",
    "state = UtilFuncs.get_state(agent, data, t, window_size + 1, utils_state)\n",
    "utils_act = [prob,data[t]]\n",
    "state[0][0:] = 0.5 \n",
    "state[0][-5] = -0.2 # bal\n",
    "state[0][-4] = -0.3 # invent\n",
    "state[0][-3] = -0.3 # port\n",
    "state[0][-2] = 0.8 # hold\n",
    "state[0][-1] = 0. # holdin\n",
    "print(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, action_prob = agent.act(state, utils_act)\n",
    "action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting RL vs buyhold\n",
    "fig = pgo.Figure()\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(growth)), y=np.array(growth),\n",
    "                    mode='lines',\n",
    "                    name='growth_RL'))\n",
    "fig.add_trace(pgo.Scatter(x=np.arange(len(growth)), y=np.array(growth_buyhold),\n",
    "                    mode='lines',\n",
    "                    name='growth_buyhold'))\n",
    "fig.add_trace(pgo.Scatter(x=buy_ind, y=data[buy_ind], marker_color = \"green\",\n",
    "                    mode='markers',\n",
    "                    name='buy'))\n",
    "fig.add_trace(pgo.Scatter(x=sell_ind, y=data[sell_ind], marker_color = \"red\",\n",
    "                    mode='markers',\n",
    "                    name='sell'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing whether model even learnt\n",
    "model1 = Agent(window_size, batch_size)\n",
    "model1 = model1.critic_target.model\n",
    "a= model1.get_weights()\n",
    "print(a[10])\n",
    "print(\"=================================\")\n",
    "model2 = Agent(window_size, batch_size)\n",
    "model2.critic_target.model.load_weights(\"./run1/e203/critic_local.h5\")\n",
    "b =model2.critic_target.model.get_weights()\n",
    "print(b[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "growth_buyhold[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf280",
   "language": "python",
   "name": "tf280"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
